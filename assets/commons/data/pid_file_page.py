"""
DO NOT EDIT! THIS IS AN AUTOGENERATED FILE!!!!!
"""

import asyncio
import functools
import itertools
import time
import traceback
from collections import Counter, defaultdict
from contextlib import nullcontext
from datetime import datetime, timezone
from typing import Dict, List, Self, Any

import numpy as np
import pandas as pd
from utils.logger import makeCustomLogger, logging_tqdm
from sqlalchemy.schema import Column
from sqlalchemy.sql import (
    delete,
    values,
    cast,
    select,
    column,
    case,
    or_,
    literal,
    and_,
)
from sqlalchemy.dialects.postgresql import insert
from sqlalchemy.exc import IntegrityError
from sqlalchemy.inspection import inspect


from core.data.SentoBase import SentoBaseData, split, request_manager
from core.database.db import Session as indexingSession
from utils.async_db import indexingAsyncSession
from utils.enums import *
from models import pid_file_page as ORMpid_file_page


class pid_file_page(SentoBaseData):
    _logger = makeCustomLogger("pid_file_page")
    _get_all_filter_meta: dict[str, dict] = {
        "pid_file_id": {"condition": "==", "column": "pid_file_id"},
        "page_number": {"condition": "==", "column": "page_number"},
    }
    _fields: list[str] = [
        "id",
        "pid_file_id",
        "page_number",
        "height",
        "width",
        "rotation",
        "image_s3_key",
    ]
    _primary_keys: list[str] = ["id"]
    _unique_fields: list[str] = ["pid_file_id", "page_number"]
    _non_unique_fields: list[str] = ["height", "image_s3_key", "rotation", "width"]
    # Only use set when ordering is not important. (Is important for bulk insert)
    _nullable_fields: set[str] = set(
        {
            "height",
            "id",
            "image_s3_key",
            "rotation",
            "width",
        }
    )
    _orm: type[ORMpid_file_page] = ORMpid_file_page

    def __init__(
        self,
        id: int = None,
        pid_file_id: int = None,
        page_number: int = None,
        height: int = None,
        width: int = None,
        rotation: int = None,
        image_s3_key: str = None,
        *args,
        **kwargs,
    ):
        super().__init__()

        if id is None:
            self.__id = None
        else:
            self.id = id
        if pid_file_id is None:
            self.__pid_file_id = None
        else:
            self.pid_file_id = pid_file_id
        if page_number is None:
            self.__page_number = None
        else:
            self.page_number = page_number
        if height is None:
            self.__height = None
        else:
            self.height = height
        if width is None:
            self.__width = None
        else:
            self.width = width
        if rotation is None:
            self.__rotation = None
        else:
            self.rotation = rotation
        if image_s3_key is None:
            self.__image_s3_key = None
        else:
            self.image_s3_key = image_s3_key

    @property
    def id(self):
        return self.__id

    @id.setter
    def id(self, new_id):
        if not hasattr(self, "__id") or new_id is not None:
            self.__id = int(new_id) if new_id is not None else None

    @property
    def pid_file_id(self):
        return self.__pid_file_id

    @pid_file_id.setter
    def pid_file_id(self, new_pid_file_id):
        if not hasattr(self, "__pid_file_id") or new_pid_file_id is not None:
            self.__pid_file_id = (
                int(new_pid_file_id) if new_pid_file_id is not None else None
            )

    @property
    def page_number(self):
        return self.__page_number

    @page_number.setter
    def page_number(self, new_page_number):
        if not hasattr(self, "__page_number") or new_page_number is not None:
            self.__page_number = (
                int(new_page_number) if new_page_number is not None else None
            )

    @property
    def height(self):
        return self.__height

    @height.setter
    def height(self, new_height):
        if not hasattr(self, "__height") or new_height is not None:
            self.__height = int(new_height) if new_height is not None else None

    @property
    def width(self):
        return self.__width

    @width.setter
    def width(self, new_width):
        if not hasattr(self, "__width") or new_width is not None:
            self.__width = int(new_width) if new_width is not None else None

    @property
    def rotation(self):
        return self.__rotation

    @rotation.setter
    def rotation(self, new_rotation):
        if not hasattr(self, "__rotation") or new_rotation is not None:
            self.__rotation = int(new_rotation) if new_rotation is not None else None

    @property
    def image_s3_key(self):
        return self.__image_s3_key

    @image_s3_key.setter
    def image_s3_key(self, new_image_s3_key):
        if not hasattr(self, "__image_s3_key") or new_image_s3_key is not None:
            self.__image_s3_key = new_image_s3_key

    @classmethod
    def get_all(cls, limit=None, db=None, **kwargs):
        try:
            filters = []
            for k, v in kwargs.items():
                filter_meta = cls._get_all_filter_meta.get(k, {})
                if v is not None:
                    if filter_meta.get("condition", "==") == "in":
                        v_split = v.split(",")
                        filter = f"ORMpid_file_page.{filter_meta.get('column', '==')}.in_(v_split)"
                    else:
                        filter = f"ORMpid_file_page.{filter_meta.get('column', '==')} {filter_meta.get('condition', '==')} v"
                    filters.append(eval(filter))
            with indexingSession() if db is None else nullcontext(db) as db:
                # items = db.query(ORMpid_file_page).filter(
                #     *(getattr(ORMpid_file_page, k) == v for k, v in kwargs.items())
                # ).all()
                items = db.query(ORMpid_file_page).filter(*filters).limit(limit).all()
        except Exception as e:
            cls._logger.exception(f"Error getting all {cls.__name__}s")
            items = []
        return [cls.from_orm(item) for item in items]

    @classmethod
    async def async_get_all(cls, limit=None, adb=None, **kwargs):
        try:
            filters = []
            for k, v in kwargs.items():
                filter_meta = cls._get_all_filter_meta.get(k, {})
                if v is not None:
                    if filter_meta.get("condition", "==") == "in":
                        v_split = v.split(",")
                        filter = f"ORMpid_file_page.{filter_meta.get('column', '==')}.in_(v_split)"
                    else:
                        filter = f"ORMpid_file_page.{filter_meta.get('column', '==')} {filter_meta.get('condition', '==')} v"
                    filters.append(eval(filter))
            async with (
                indexingAsyncSession() if adb is None else nullcontext(adb)
            ) as adb:
                # items = db.query(ORMpid_file_page).filter(
                #     *(getattr(ORMpid_file_page, k) == v for k, v in kwargs.items())
                # ).all()
                stmt = select(ORMpid_file_page).filter(*filters).limit(limit)
                items = (await adb.execute(stmt)).scalars().all()
        except Exception as e:
            cls._logger.exception(f"Error getting all {cls.__name__}s")
            items = []
        return [cls.from_orm(item) for item in items]

    @classmethod
    def get(cls, **kwargs):
        data = cls.get_all(**kwargs)
        if len(data) > 0:
            if len(data) > 1:
                cls._logger.warning(
                    "More than one result found, only returning the first.."
                )
            return data[0]
        else:
            return None

    @classmethod
    async def async_get(cls, **kwargs):
        data = await cls.async_get_all(**kwargs)
        if len(data) > 0:
            if len(data) > 1:
                cls._logger.warning(
                    "More than one result found, only returning the first.."
                )
            return data[0]
        else:
            return None

    @classmethod
    def get_or_create(cls, **kwargs):
        data = cls.get(**kwargs)
        if data:
            return data
        else:
            return cls(**kwargs)

    @classmethod
    def from_id(cls, id: int, db=None):
        try:
            with indexingSession() if db is None else nullcontext(db) as db:
                data = db.get(ORMpid_file_page, id)
            if data:
                return cls.from_orm(data)
            else:
                return None
        except Exception as e:
            cls._logger.exception(f"Error getting id {id} ({cls.__name__})")
            return None

    @classmethod
    async def async_from_id(cls, id: int, adb=None):
        try:
            async with (
                indexingAsyncSession() if adb is None else nullcontext(adb)
            ) as adb:
                data = await adb.get(ORMpid_file_page, id)
            if data:
                return cls.from_orm(data)
            else:
                return None
        except Exception as e:
            cls._logger.exception(f"Error getting id {id} ({cls.__name__})")
            return None

    @classmethod
    def from_dict(cls, new_obj: Dict):
        if not {"pid_file_id", "page_number"}.issubset(new_obj.keys()):
            raise KeyError("dict should contain at least pid_file_id,page_number")
        return cls(**new_obj)

    def create(self):
        self.save()
        self._logger.debug(f"Created new pid_file_page with id: {self.id}")
        # try:
        #    data = self.create_all([self])
        # except Exception as e:
        #    capture_exception(e)
        #    data = None
        #    raise e
        # if data is not None:
        #    if len(data) > 0:
        #        self.__id = data[0].get("id")

    async def async_create(self):
        await self.async_save()
        self._logger.debug(f"Created new pid_file_page with id: {self.id}")

    @classmethod
    def bulk_upsert(cls, items: list[Self], db):
        unique_fields = cls._unique_fields if cls._unique_fields else cls._primary_keys
        if len(items) == 0:
            return []
        res = []
        for batch in itertools.batched(items, 20000):
            c = Counter()
            pre_stmt, stmt = cls.make_upsert_statement(batch)
            for x in pre_stmt:
                db.execute(x)
            r = (db.execute(stmt)).all()
            idmap = {}  # used to return the initial ordering
            for x in r:
                c[x.source] += 1
                idmap[tuple(getattr(x, k) for k in unique_fields)] = x.id
            cls._logger.info(
                f"Upserted {len(batch)} {cls.__name__}: {c['inserted']} inserts, {c['updated']} updates, {c['selected']} no-op"
            )
            r = [idmap[tuple(getattr(x, k) for k in unique_fields)] for x in batch]
            res.extend(r)
        return res

    @classmethod
    async def async_bulk_upsert(cls, items: list[Self], adb) -> list[int]:
        unique_fields = cls.unique_fields
        if len(items) == 0:
            return []
        res, res_map = [], {}
        # Sort for consistent ordering and avoiding deadlocks
        initial_order = [tuple([getattr(x, k) for k in unique_fields]) for x in items]
        items = sorted(items, key=lambda x: [getattr(x, k) for k in unique_fields])
        for batch in itertools.batched(items, 20000):
            c = Counter()
            pre_stmt, stmt = cls.make_upsert_statement(batch)
            for x in pre_stmt:
                await adb.execute(x)
            r = (await adb.execute(stmt)).all()
            idmap = {}  # used to return the initial ordering
            for x in r:
                c[x.source] += 1
                idmap[tuple(getattr(x, k) for k in unique_fields)] = x.id
            if len(idmap) < len(batch):
                # In a concurrent environment, concurrent insert might be executed at
                # the same time as this upsert statement. In that case, the execution
                # might not return a row because (1. it got inserted from a concurrent
                # transaction, and 2. the concurrent transaction hasn't committed yet)
                # Due to concurrent inserts, we might have to execute query to get the missing rows
                retries = 0
                while len(idmap) < len(batch):
                    await asyncio.sleep(10)
                    missing = [
                        x
                        for x in batch
                        if tuple(getattr(x, k) for k in unique_fields) not in idmap
                    ]
                    if not missing:
                        break
                    pre_stmt, stmt = cls.make_upsert_statement(missing)
                    for x in pre_stmt:
                        await adb.execute(x)
                    r = (await adb.execute(stmt)).all()
                    for x in r:
                        c[x.source] += 1
                        idmap[tuple(getattr(x, k) for k in unique_fields)] = x.id
                    retries += 1
                    if retries > 5:
                        cls._logger.warning(
                            f"Retried upsert {retries} times, but still missing rows"
                        )
                        break
            cls._logger.info(
                f"Upserted {len(batch)} {cls.__name__}: {c['inserted']} inserts, {c['updated']} updates, {c['selected']} no-op"
            )
            r = [idmap[tuple(getattr(x, k) for k in unique_fields)] for x in batch]
            res.extend(r)
            res_map.update(idmap)
        # Ensure correct ordering is returned using initial_order
        return [res_map[tupl] for tupl in initial_order]

    @classmethod
    def from_orm(cls, orm: ORMpid_file_page):
        return cls(
            **{
                field.name: getattr(orm, field.name)
                for field in inspect(ORMpid_file_page).c
                if isinstance(field, Column)
            }
        )

    def to_orm(self, db, safe=False):
        d = self.to_create_dict()
        if self.id:
            self_ORM = db.query(ORMpid_file_page).with_for_update().get(self.id)
        else:
            del d["id"]
            if safe:
                unique_data = {k: v for k, v in d.items() if k in self._unique_fields}
                self_ORM = (
                    db.query(ORMpid_file_page)
                    .filter_by(**unique_data)
                    .with_for_update()
                    .one_or_none()
                )
            else:
                self_ORM = None

        if not self_ORM:
            self_ORM = ORMpid_file_page(**d)
        else:
            for key, value in d.items():
                if key not in self._unique_fields:
                    setattr(self_ORM, key, value)

        return self_ORM

    async def async_to_orm(self, adb, safe=False):
        d = self.to_create_dict()
        if self.id:
            self_ORM = await adb.get(ORMpid_file_page, self.id, with_for_update=True)
        else:
            del d["id"]
            if safe:
                unique_data = {k: v for k, v in d.items() if k in self._unique_fields}
                stmt = (
                    select(ORMpid_file_page).filter_by(**unique_data).with_for_update()
                )
                self_ORM = (await adb.execute(stmt)).scalar_one_or_none()
            else:
                self_ORM = None

        if not self_ORM:
            self_ORM = ORMpid_file_page(**d)
        else:
            for key, value in d.items():
                if key not in self._unique_fields:
                    setattr(self_ORM, key, value)

        return self_ORM

    def unsafe_safe_save(self, db=None, safe=False):
        commit = False if db else True
        unique_fields = self._unique_fields
        with indexingSession() if not db else nullcontext(db) as db:
            if self.id:
                # update
                self_ORM = db.get(ORMpid_file_page, self.id)
                for key, value in self.to_create_dict().items():
                    setattr(self_ORM, key, value)
            elif len(unique_fields) > 0:
                # check if exists
                d = self.to_create_dict()
                del d["id"]
                unique_data = {k: v for k, v in d.items() if k in unique_fields}
                self_ORM = (
                    db.query(ORMpid_file_page)
                    .filter_by(**unique_data)
                    .with_for_update()
                    .one_or_none()
                )
                if self_ORM:
                    for key, value in d.items():
                        setattr(self_ORM, key, value)
                else:
                    self_ORM = ORMpid_file_page(**d)
                    db.add(self_ORM)
                    db.commit()  # commit here to get the id
                self.__id = self_ORM.id
            else:
                d = self.to_create_dict()
                del d["id"]
                self_ORM = ORMpid_file_page(**d)
                db.add(self_ORM)
                db.commit()  # commit here to get the id
                self.__id = self_ORM.id

            if commit:
                self._logger.debug(
                    f"Committing pid_file_page({self.id}) to db. New: {len(db.new)}, Dirty: {len(db.dirty)}, Deleted: {len(db.deleted)}"
                )
                db.commit()
        return self

    async def async_unsafe_safe_save(self, adb=None, safe=False):
        commit = False if adb else True
        unique_fields = self._unique_fields
        async with indexingAsyncSession() if not adb else nullcontext(adb) as adb:
            if self.id:
                # update
                self_ORM = await adb.get(ORMpid_file_page, self.id)
                for key, value in self.to_create_dict().items():
                    setattr(self_ORM, key, value)
            elif len(unique_fields) > 0:
                # check if exists
                d = self.to_create_dict()
                del d["id"]
                unique_data = {k: v for k, v in d.items() if k in unique_fields}
                stmt = (
                    select(ORMpid_file_page).filter_by(**unique_data).with_for_update()
                )
                self_ORM = (await adb.execute(stmt)).scalar_one_or_none()
                if self_ORM:
                    for key, value in d.items():
                        setattr(self_ORM, key, value)
                else:
                    self_ORM = ORMpid_file_page(**d)
                    adb.add(self_ORM)
                    await adb.flush()  # flush here to get the id
                self.__id = self_ORM.id
            else:
                d = self.to_create_dict()
                del d["id"]
                self_ORM = ORMpid_file_page(**d)
                adb.add(self_ORM)
                await adb.flush()  # flush here to get the id
                self.__id = self_ORM.id

            if commit:
                self._logger.debug(
                    f"Committing pid_file_page({self.id}) to db. New: {len(adb.new)}, Dirty: {len(adb.dirty)}, Deleted: {len(adb.deleted)}"
                )
                await adb.commit()
        return self

    def save(self, db=None):
        start = time.time()
        try:
            res = self.unsafe_safe_save(db=db)
        except IntegrityError as e:
            self._logger.warning(
                f"IntegrityError while saving pid_file_page({self.id}) to db. Trying safe save."
            )
            res = self.unsafe_safe_save(db=db, safe=True)
        self._logger.info(
            f"Saved pid_file_page({self.id}) to db in {time.time() - start:.3f} seconds"
        )
        return res

    async def async_save(self, adb=None):
        start = time.time()
        try:
            res = await self.async_unsafe_safe_save(adb=adb)
        except IntegrityError as e:
            self._logger.warning(
                f"IntegrityError while saving pid_file_page({self.id}) to db. Trying safe save."
            )
            res = await self.async_unsafe_safe_save(adb=adb, safe=True)
        self._logger.info(
            f"Saved pid_file_page({self.id}) to db in {time.time() - start:.3f} seconds"
        )
        return res

    @classmethod
    def create_all(cls, items, fk_column=None, fk_id=None):
        path = f"/pid_file_page/all"
        all_data = []
        try:
            for chunk in logging_tqdm(
                iterable=split(items, 100),
                desc="saving to pid_file_page",
                mininterval=10,
                total=1 + (len(items) - 1) // 100,
                leave=False,
                logger=cls._logger,
            ):
                chunk_items = [item.to_create_dict() for item in chunk]
                if fk_id is not None:

                    def set_fk_id(item, column, id):
                        item[column] = id
                        return item

                    chunk_items = [
                        set_fk_id(item, fk_column, fk_id) for item in chunk_items
                    ]
                chunk_data = request_manager.post(path=path, data=chunk_items)
                all_data = all_data + chunk_data

            for idx, (saved_item, item) in logging_tqdm(
                enumerate(zip(all_data, items)),
                desc="deleting old children",
                mininterval=10,
                total=len(items),
                leave=False,
                logger=cls._logger,
            ):
                _id = saved_item.get("id")
            starting_position = 0
        except Exception as e:
            cls._logger.exception(f"Error creating all ({cls.__name__})")
            all_data = None
            raise e
        return all_data

    def delete(self, db=None):
        if not self.__id:
            self.__set_id()
        if self.__id:
            try:
                with indexingSession() if db is None else nullcontext(db) as db:
                    data = db.get(ORMpid_file_page, self.__id)
                    db.delete(data)
                    db.commit()
                self.__id = None
            except Exception as e:
                self._logger.exception(f"Error deleting {self.__id} ({self.__name__})")

        else:
            self._logger.info("Object cannot be deleted as it does not exist yet")

    async def async_delete(self, adb=None):
        if not self.__id:
            self.__set_id()
        if self.__id:
            try:
                async with (
                    indexingAsyncSession() if adb is None else nullcontext(adb)
                ) as adb:
                    data = await adb.get(ORMpid_file_page, self.__id)
                    await adb.delete(data)
                    await adb.commit()
                self.__id = None
            except Exception as e:
                self._logger.exception(f"Error deleting {self.__id} ({self.__name__})")

        else:
            self._logger.info("Object cannot be deleted as it does not exist yet")

    def to_dict(self):
        return dict(
            id=self.__id,
            pid_file_id=self.__pid_file_id,
            page_number=self.__page_number,
            height=self.__height,
            width=self.__width,
            rotation=self.__rotation,
            image_s3_key=self.__image_s3_key,
        )

    def to_create_dict(self):
        return dict(
            id=self.__id,
            pid_file_id=self.__pid_file_id,
            page_number=self.__page_number,
            height=self.__height,
            width=self.__width,
            rotation=self.__rotation,
            image_s3_key=self.__image_s3_key,
            modified_on=datetime.now(),
        )

    def to_update_dict(self):
        return dict(
            height=self.__height,
            width=self.__width,
            rotation=self.__rotation,
            image_s3_key=self.__image_s3_key,
            modified_on=datetime.now(),
        )
