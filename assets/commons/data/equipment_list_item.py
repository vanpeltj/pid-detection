"""
DO NOT EDIT! THIS IS AN AUTOGENERATED FILE!!!!!
"""

import asyncio
import functools
import itertools
import time
import traceback
from collections import Counter, defaultdict
from contextlib import nullcontext
from datetime import datetime, timezone
from typing import Dict, List, Self, Any

import numpy as np
import pandas as pd
from utils.logger import makeCustomLogger, logging_tqdm
from sqlalchemy.schema import Column
from sqlalchemy.sql import (
    delete,
    values,
    cast,
    select,
    column,
    case,
    or_,
    literal,
    and_,
)
from sqlalchemy.dialects.postgresql import insert
from sqlalchemy.exc import IntegrityError
from sqlalchemy.inspection import inspect


from core.data.SentoBase import SentoBaseData, split, request_manager
from core.database.db import Session as indexingSession
from utils.async_db import indexingAsyncSession
from utils.enums import *
from models import equipment_list_item as ORMequipment_list_item


class equipment_list_item(SentoBaseData):
    _logger = makeCustomLogger("equipment_list_item")
    _get_all_filter_meta: dict[str, dict] = {
        "equipment_list_id": {"condition": "==", "column": "equipment_list_id"},
        "row_id": {"condition": "==", "column": "row_id"},
        "column_id": {"condition": "==", "column": "column_id"},
        "field": {"condition": "==", "column": "field"},
    }
    _fields: list[str] = [
        "id",
        "equipment_list_id",
        "row_id",
        "column_id",
        "field",
        "value",
    ]
    _primary_keys: list[str] = ["id"]
    _unique_fields: list[str] = ["equipment_list_id", "row_id", "column_id", "field"]
    _non_unique_fields: list[str] = ["value"]
    # Only use set when ordering is not important. (Is important for bulk insert)
    _nullable_fields: set[str] = set(
        {
            "field",
            "id",
            "value",
        }
    )
    _orm: type[ORMequipment_list_item] = ORMequipment_list_item

    def __init__(
        self,
        id: int = None,
        equipment_list_id: int = None,
        row_id: int = None,
        column_id: int = None,
        field: str = None,
        value: str = None,
        *args,
        **kwargs,
    ):
        super().__init__()

        if id is None:
            self.__id = None
        else:
            self.id = id
        if equipment_list_id is None:
            self.__equipment_list_id = None
        else:
            self.equipment_list_id = equipment_list_id
        if row_id is None:
            self.__row_id = None
        else:
            self.row_id = row_id
        if column_id is None:
            self.__column_id = None
        else:
            self.column_id = column_id
        if field is None:
            self.__field = None
        else:
            self.field = field
        if value is None:
            self.__value = None
        else:
            self.value = value

    @property
    def id(self):
        return self.__id

    @id.setter
    def id(self, new_id):
        if not hasattr(self, "__id") or new_id is not None:
            self.__id = int(new_id) if new_id is not None else None

    @property
    def equipment_list_id(self):
        return self.__equipment_list_id

    @equipment_list_id.setter
    def equipment_list_id(self, new_equipment_list_id):
        if (
            not hasattr(self, "__equipment_list_id")
            or new_equipment_list_id is not None
        ):
            self.__equipment_list_id = (
                int(new_equipment_list_id)
                if new_equipment_list_id is not None
                else None
            )

    @property
    def row_id(self):
        return self.__row_id

    @row_id.setter
    def row_id(self, new_row_id):
        if not hasattr(self, "__row_id") or new_row_id is not None:
            self.__row_id = int(new_row_id) if new_row_id is not None else None

    @property
    def column_id(self):
        return self.__column_id

    @column_id.setter
    def column_id(self, new_column_id):
        if not hasattr(self, "__column_id") or new_column_id is not None:
            self.__column_id = int(new_column_id) if new_column_id is not None else None

    @property
    def field(self):
        return self.__field

    @field.setter
    def field(self, new_field):
        if not hasattr(self, "__field") or new_field is not None:
            self.__field = new_field

    @property
    def value(self):
        return self.__value

    @value.setter
    def value(self, new_value):
        if not hasattr(self, "__value") or new_value is not None:
            self.__value = new_value

    @classmethod
    def get_all(cls, limit=None, db=None, **kwargs):
        try:
            filters = []
            for k, v in kwargs.items():
                filter_meta = cls._get_all_filter_meta.get(k, {})
                if v is not None:
                    if filter_meta.get("condition", "==") == "in":
                        v_split = v.split(",")
                        filter = f"ORMequipment_list_item.{filter_meta.get('column', '==')}.in_(v_split)"
                    else:
                        filter = f"ORMequipment_list_item.{filter_meta.get('column', '==')} {filter_meta.get('condition', '==')} v"
                    filters.append(eval(filter))
            with indexingSession() if db is None else nullcontext(db) as db:
                # items = db.query(ORMequipment_list_item).filter(
                #     *(getattr(ORMequipment_list_item, k) == v for k, v in kwargs.items())
                # ).all()
                items = (
                    db.query(ORMequipment_list_item).filter(*filters).limit(limit).all()
                )
        except Exception as e:
            cls._logger.exception(f"Error getting all {cls.__name__}s")
            items = []
        return [cls.from_orm(item) for item in items]

    @classmethod
    async def async_get_all(cls, limit=None, adb=None, **kwargs):
        try:
            filters = []
            for k, v in kwargs.items():
                filter_meta = cls._get_all_filter_meta.get(k, {})
                if v is not None:
                    if filter_meta.get("condition", "==") == "in":
                        v_split = v.split(",")
                        filter = f"ORMequipment_list_item.{filter_meta.get('column', '==')}.in_(v_split)"
                    else:
                        filter = f"ORMequipment_list_item.{filter_meta.get('column', '==')} {filter_meta.get('condition', '==')} v"
                    filters.append(eval(filter))
            async with (
                indexingAsyncSession() if adb is None else nullcontext(adb)
            ) as adb:
                # items = db.query(ORMequipment_list_item).filter(
                #     *(getattr(ORMequipment_list_item, k) == v for k, v in kwargs.items())
                # ).all()
                stmt = select(ORMequipment_list_item).filter(*filters).limit(limit)
                items = (await adb.execute(stmt)).scalars().all()
        except Exception as e:
            cls._logger.exception(f"Error getting all {cls.__name__}s")
            items = []
        return [cls.from_orm(item) for item in items]

    @classmethod
    def get(cls, **kwargs):
        data = cls.get_all(**kwargs)
        if len(data) > 0:
            if len(data) > 1:
                cls._logger.warning(
                    "More than one result found, only returning the first.."
                )
            return data[0]
        else:
            return None

    @classmethod
    async def async_get(cls, **kwargs):
        data = await cls.async_get_all(**kwargs)
        if len(data) > 0:
            if len(data) > 1:
                cls._logger.warning(
                    "More than one result found, only returning the first.."
                )
            return data[0]
        else:
            return None

    @classmethod
    def get_or_create(cls, **kwargs):
        data = cls.get(**kwargs)
        if data:
            return data
        else:
            return cls(**kwargs)

    @classmethod
    def from_id(cls, id: int, db=None):
        try:
            with indexingSession() if db is None else nullcontext(db) as db:
                data = db.get(ORMequipment_list_item, id)
            if data:
                return cls.from_orm(data)
            else:
                return None
        except Exception as e:
            cls._logger.exception(f"Error getting id {id} ({cls.__name__})")
            return None

    @classmethod
    async def async_from_id(cls, id: int, adb=None):
        try:
            async with (
                indexingAsyncSession() if adb is None else nullcontext(adb)
            ) as adb:
                data = await adb.get(ORMequipment_list_item, id)
            if data:
                return cls.from_orm(data)
            else:
                return None
        except Exception as e:
            cls._logger.exception(f"Error getting id {id} ({cls.__name__})")
            return None

    @classmethod
    def from_dict(cls, new_obj: Dict):
        if not {"equipment_list_id", "row_id", "column_id", "field"}.issubset(
            new_obj.keys()
        ):
            raise KeyError(
                "dict should contain at least equipment_list_id,row_id,column_id,field"
            )
        return cls(**new_obj)

    def create(self):
        self.save()
        self._logger.debug(f"Created new equipment_list_item with id: {self.id}")
        # try:
        #    data = self.create_all([self])
        # except Exception as e:
        #    capture_exception(e)
        #    data = None
        #    raise e
        # if data is not None:
        #    if len(data) > 0:
        #        self.__id = data[0].get("id")

    async def async_create(self):
        await self.async_save()
        self._logger.debug(f"Created new equipment_list_item with id: {self.id}")

    @classmethod
    def bulk_upsert(cls, items: list[Self], db):
        unique_fields = cls._unique_fields if cls._unique_fields else cls._primary_keys
        if len(items) == 0:
            return []
        res = []
        for batch in itertools.batched(items, 20000):
            c = Counter()
            pre_stmt, stmt = cls.make_upsert_statement(batch)
            for x in pre_stmt:
                db.execute(x)
            r = (db.execute(stmt)).all()
            idmap = {}  # used to return the initial ordering
            for x in r:
                c[x.source] += 1
                idmap[tuple(getattr(x, k) for k in unique_fields)] = x.id
            cls._logger.info(
                f"Upserted {len(batch)} {cls.__name__}: {c['inserted']} inserts, {c['updated']} updates, {c['selected']} no-op"
            )
            r = [idmap[tuple(getattr(x, k) for k in unique_fields)] for x in batch]
            res.extend(r)
        return res

    @classmethod
    async def async_bulk_upsert(cls, items: list[Self], adb) -> list[int]:
        unique_fields = cls.unique_fields
        if len(items) == 0:
            return []
        res, res_map = [], {}
        # Sort for consistent ordering and avoiding deadlocks
        initial_order = [tuple([getattr(x, k) for k in unique_fields]) for x in items]
        items = sorted(items, key=lambda x: [getattr(x, k) for k in unique_fields])
        for batch in itertools.batched(items, 20000):
            c = Counter()
            pre_stmt, stmt = cls.make_upsert_statement(batch)
            for x in pre_stmt:
                await adb.execute(x)
            r = (await adb.execute(stmt)).all()
            idmap = {}  # used to return the initial ordering
            for x in r:
                c[x.source] += 1
                idmap[tuple(getattr(x, k) for k in unique_fields)] = x.id
            if len(idmap) < len(batch):
                # In a concurrent environment, concurrent insert might be executed at
                # the same time as this upsert statement. In that case, the execution
                # might not return a row because (1. it got inserted from a concurrent
                # transaction, and 2. the concurrent transaction hasn't committed yet)
                # Due to concurrent inserts, we might have to execute query to get the missing rows
                retries = 0
                while len(idmap) < len(batch):
                    await asyncio.sleep(10)
                    missing = [
                        x
                        for x in batch
                        if tuple(getattr(x, k) for k in unique_fields) not in idmap
                    ]
                    if not missing:
                        break
                    pre_stmt, stmt = cls.make_upsert_statement(missing)
                    for x in pre_stmt:
                        await adb.execute(x)
                    r = (await adb.execute(stmt)).all()
                    for x in r:
                        c[x.source] += 1
                        idmap[tuple(getattr(x, k) for k in unique_fields)] = x.id
                    retries += 1
                    if retries > 5:
                        cls._logger.warning(
                            f"Retried upsert {retries} times, but still missing rows"
                        )
                        break
            cls._logger.info(
                f"Upserted {len(batch)} {cls.__name__}: {c['inserted']} inserts, {c['updated']} updates, {c['selected']} no-op"
            )
            r = [idmap[tuple(getattr(x, k) for k in unique_fields)] for x in batch]
            res.extend(r)
            res_map.update(idmap)
        # Ensure correct ordering is returned using initial_order
        return [res_map[tupl] for tupl in initial_order]

    @classmethod
    def from_orm(cls, orm: ORMequipment_list_item):
        return cls(
            **{
                field.name: getattr(orm, field.name)
                for field in inspect(ORMequipment_list_item).c
                if isinstance(field, Column)
            }
        )

    def to_orm(self, db, safe=False):
        d = self.to_create_dict()
        if self.id:
            self_ORM = db.query(ORMequipment_list_item).with_for_update().get(self.id)
        else:
            del d["id"]
            if safe:
                unique_data = {k: v for k, v in d.items() if k in self._unique_fields}
                self_ORM = (
                    db.query(ORMequipment_list_item)
                    .filter_by(**unique_data)
                    .with_for_update()
                    .one_or_none()
                )
            else:
                self_ORM = None

        if not self_ORM:
            self_ORM = ORMequipment_list_item(**d)
        else:
            for key, value in d.items():
                if key not in self._unique_fields:
                    setattr(self_ORM, key, value)

        return self_ORM

    async def async_to_orm(self, adb, safe=False):
        d = self.to_create_dict()
        if self.id:
            self_ORM = await adb.get(
                ORMequipment_list_item, self.id, with_for_update=True
            )
        else:
            del d["id"]
            if safe:
                unique_data = {k: v for k, v in d.items() if k in self._unique_fields}
                stmt = (
                    select(ORMequipment_list_item)
                    .filter_by(**unique_data)
                    .with_for_update()
                )
                self_ORM = (await adb.execute(stmt)).scalar_one_or_none()
            else:
                self_ORM = None

        if not self_ORM:
            self_ORM = ORMequipment_list_item(**d)
        else:
            for key, value in d.items():
                if key not in self._unique_fields:
                    setattr(self_ORM, key, value)

        return self_ORM

    def unsafe_safe_save(self, db=None, safe=False):
        commit = False if db else True
        unique_fields = self._unique_fields
        with indexingSession() if not db else nullcontext(db) as db:
            if self.id:
                # update
                self_ORM = db.get(ORMequipment_list_item, self.id)
                for key, value in self.to_create_dict().items():
                    setattr(self_ORM, key, value)
            elif len(unique_fields) > 0:
                # check if exists
                d = self.to_create_dict()
                del d["id"]
                unique_data = {k: v for k, v in d.items() if k in unique_fields}
                self_ORM = (
                    db.query(ORMequipment_list_item)
                    .filter_by(**unique_data)
                    .with_for_update()
                    .one_or_none()
                )
                if self_ORM:
                    for key, value in d.items():
                        setattr(self_ORM, key, value)
                else:
                    self_ORM = ORMequipment_list_item(**d)
                    db.add(self_ORM)
                    db.commit()  # commit here to get the id
                self.__id = self_ORM.id
            else:
                d = self.to_create_dict()
                del d["id"]
                self_ORM = ORMequipment_list_item(**d)
                db.add(self_ORM)
                db.commit()  # commit here to get the id
                self.__id = self_ORM.id

            if commit:
                self._logger.debug(
                    f"Committing equipment_list_item({self.id}) to db. New: {len(db.new)}, Dirty: {len(db.dirty)}, Deleted: {len(db.deleted)}"
                )
                db.commit()
        return self

    async def async_unsafe_safe_save(self, adb=None, safe=False):
        commit = False if adb else True
        unique_fields = self._unique_fields
        async with indexingAsyncSession() if not adb else nullcontext(adb) as adb:
            if self.id:
                # update
                self_ORM = await adb.get(ORMequipment_list_item, self.id)
                for key, value in self.to_create_dict().items():
                    setattr(self_ORM, key, value)
            elif len(unique_fields) > 0:
                # check if exists
                d = self.to_create_dict()
                del d["id"]
                unique_data = {k: v for k, v in d.items() if k in unique_fields}
                stmt = (
                    select(ORMequipment_list_item)
                    .filter_by(**unique_data)
                    .with_for_update()
                )
                self_ORM = (await adb.execute(stmt)).scalar_one_or_none()
                if self_ORM:
                    for key, value in d.items():
                        setattr(self_ORM, key, value)
                else:
                    self_ORM = ORMequipment_list_item(**d)
                    adb.add(self_ORM)
                    await adb.flush()  # flush here to get the id
                self.__id = self_ORM.id
            else:
                d = self.to_create_dict()
                del d["id"]
                self_ORM = ORMequipment_list_item(**d)
                adb.add(self_ORM)
                await adb.flush()  # flush here to get the id
                self.__id = self_ORM.id

            if commit:
                self._logger.debug(
                    f"Committing equipment_list_item({self.id}) to db. New: {len(adb.new)}, Dirty: {len(adb.dirty)}, Deleted: {len(adb.deleted)}"
                )
                await adb.commit()
        return self

    def save(self, db=None):
        start = time.time()
        try:
            res = self.unsafe_safe_save(db=db)
        except IntegrityError as e:
            self._logger.warning(
                f"IntegrityError while saving equipment_list_item({self.id}) to db. Trying safe save."
            )
            res = self.unsafe_safe_save(db=db, safe=True)
        self._logger.info(
            f"Saved equipment_list_item({self.id}) to db in {time.time() - start:.3f} seconds"
        )
        return res

    async def async_save(self, adb=None):
        start = time.time()
        try:
            res = await self.async_unsafe_safe_save(adb=adb)
        except IntegrityError as e:
            self._logger.warning(
                f"IntegrityError while saving equipment_list_item({self.id}) to db. Trying safe save."
            )
            res = await self.async_unsafe_safe_save(adb=adb, safe=True)
        self._logger.info(
            f"Saved equipment_list_item({self.id}) to db in {time.time() - start:.3f} seconds"
        )
        return res

    @classmethod
    def create_all(cls, items, fk_column=None, fk_id=None):
        path = f"/equipment/all"
        all_data = []
        try:
            for chunk in logging_tqdm(
                iterable=split(items, 100),
                desc="saving to equipment_list_item",
                mininterval=10,
                total=1 + (len(items) - 1) // 100,
                leave=False,
                logger=cls._logger,
            ):
                chunk_items = [item.to_create_dict() for item in chunk]
                if fk_id is not None:

                    def set_fk_id(item, column, id):
                        item[column] = id
                        return item

                    chunk_items = [
                        set_fk_id(item, fk_column, fk_id) for item in chunk_items
                    ]
                chunk_data = request_manager.post(path=path, data=chunk_items)
                all_data = all_data + chunk_data

            for idx, (saved_item, item) in logging_tqdm(
                enumerate(zip(all_data, items)),
                desc="deleting old children",
                mininterval=10,
                total=len(items),
                leave=False,
                logger=cls._logger,
            ):
                _id = saved_item.get("id")
            starting_position = 0
        except Exception as e:
            cls._logger.exception(f"Error creating all ({cls.__name__})")
            all_data = None
            raise e
        return all_data

    def delete(self, db=None):
        if not self.__id:
            self.__set_id()
        if self.__id:
            try:
                with indexingSession() if db is None else nullcontext(db) as db:
                    data = db.get(ORMequipment_list_item, self.__id)
                    db.delete(data)
                    db.commit()
                self.__id = None
            except Exception as e:
                self._logger.exception(f"Error deleting {self.__id} ({self.__name__})")

        else:
            self._logger.info("Object cannot be deleted as it does not exist yet")

    async def async_delete(self, adb=None):
        if not self.__id:
            self.__set_id()
        if self.__id:
            try:
                async with (
                    indexingAsyncSession() if adb is None else nullcontext(adb)
                ) as adb:
                    data = await adb.get(ORMequipment_list_item, self.__id)
                    await adb.delete(data)
                    await adb.commit()
                self.__id = None
            except Exception as e:
                self._logger.exception(f"Error deleting {self.__id} ({self.__name__})")

        else:
            self._logger.info("Object cannot be deleted as it does not exist yet")

    def to_dict(self):
        return dict(
            id=self.__id,
            equipment_list_id=self.__equipment_list_id,
            row_id=self.__row_id,
            column_id=self.__column_id,
            field=self.__field,
            value=self.__value,
        )

    def to_create_dict(self):
        return dict(
            id=self.__id,
            equipment_list_id=self.__equipment_list_id,
            row_id=self.__row_id,
            column_id=self.__column_id,
            field=self.__field,
            value=self.__value,
            modified_on=datetime.now(),
        )

    def to_update_dict(self):
        return dict(value=self.__value, modified_on=datetime.now())
