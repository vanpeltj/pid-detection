"""
DO NOT EDIT! THIS IS AN AUTOGENERATED FILE!!!!!
"""

import asyncio
import functools
import itertools
import time
import traceback
from collections import Counter, defaultdict
from contextlib import nullcontext
from datetime import datetime, timezone
from typing import Dict, List, Self, Any

import numpy as np
import pandas as pd
from utils.logger import makeCustomLogger, logging_tqdm
from sqlalchemy.schema import Column
from sqlalchemy.sql import (
    delete,
    values,
    cast,
    select,
    column,
    case,
    or_,
    literal,
    and_,
)
from sqlalchemy.dialects.postgresql import insert
from sqlalchemy.exc import IntegrityError
from sqlalchemy.inspection import inspect


from core.data.SentoBase import SentoBaseData, split, request_manager
from core.database.db import Session as indexingSession
from utils.async_db import indexingAsyncSession
from utils.enums import *
from models import pid_file as ORMpid_file


class pid_file(SentoBaseData):
    _logger = makeCustomLogger("pid_file")
    _get_all_filter_meta: dict[str, dict] = {
        "file_uuid": {"condition": "==", "column": "file_uuid"},
        "project_id": {"condition": "==", "column": "project_id"},
    }
    _fields: list[str] = [
        "id",
        "project_id",
        "version",
        "file_name",
        "file_uuid",
        "technical_name",
        "s3_key",
    ]
    _primary_keys: list[str] = ["id"]
    _unique_fields: list[str] = ["file_uuid"]
    _non_unique_fields: list[str] = [
        "file_name",
        "project_id",
        "s3_key",
        "technical_name",
        "version",
    ]
    # Only use set when ordering is not important. (Is important for bulk insert)
    _nullable_fields: set[str] = set(
        {
            "file_name",
            "file_uuid",
            "id",
            "project_id",
            "s3_key",
            "technical_name",
            "version",
        }
    )
    _orm: type[ORMpid_file] = ORMpid_file

    def __init__(
        self,
        id: int = None,
        project_id: int = None,
        version: str = None,
        file_name: str = None,
        file_uuid: str = None,
        technical_name: str = None,
        s3_key: str = None,
        *args,
        **kwargs,
    ):
        super().__init__()

        if id is None:
            self.__id = None
        else:
            self.id = id
        if project_id is None:
            self.__project_id = None
        else:
            self.project_id = project_id
        if version is None:
            self.__version = None
        else:
            self.version = version
        if file_name is None:
            self.__file_name = None
        else:
            self.file_name = file_name
        if file_uuid is None:
            self.__file_uuid = None
        else:
            self.file_uuid = file_uuid
        if technical_name is None:
            self.__technical_name = None
        else:
            self.technical_name = technical_name
        if s3_key is None:
            self.__s3_key = None
        else:
            self.s3_key = s3_key

    @property
    def id(self):
        return self.__id

    @id.setter
    def id(self, new_id):
        if not hasattr(self, "__id") or new_id is not None:
            self.__id = int(new_id) if new_id is not None else None

    @property
    def project_id(self):
        return self.__project_id

    @project_id.setter
    def project_id(self, new_project_id):
        if not hasattr(self, "__project_id") or new_project_id is not None:
            self.__project_id = (
                int(new_project_id) if new_project_id is not None else None
            )

    @property
    def version(self):
        return self.__version

    @version.setter
    def version(self, new_version):
        if not hasattr(self, "__version") or new_version is not None:
            self.__version = new_version

    @property
    def file_name(self):
        return self.__file_name

    @file_name.setter
    def file_name(self, new_file_name):
        if not hasattr(self, "__file_name") or new_file_name is not None:
            self.__file_name = new_file_name

    @property
    def file_uuid(self):
        return self.__file_uuid

    @file_uuid.setter
    def file_uuid(self, new_file_uuid):
        if not hasattr(self, "__file_uuid") or new_file_uuid is not None:
            self.__file_uuid = new_file_uuid

    @property
    def technical_name(self):
        return self.__technical_name

    @technical_name.setter
    def technical_name(self, new_technical_name):
        if not hasattr(self, "__technical_name") or new_technical_name is not None:
            self.__technical_name = new_technical_name

    @property
    def s3_key(self):
        return self.__s3_key

    @s3_key.setter
    def s3_key(self, new_s3_key):
        if not hasattr(self, "__s3_key") or new_s3_key is not None:
            self.__s3_key = new_s3_key

    @classmethod
    def get_all(cls, limit=None, db=None, **kwargs):
        try:
            filters = []
            for k, v in kwargs.items():
                filter_meta = cls._get_all_filter_meta.get(k, {})
                if v is not None:
                    if filter_meta.get("condition", "==") == "in":
                        v_split = v.split(",")
                        filter = f"ORMpid_file.{filter_meta.get('column', '==')}.in_(v_split)"
                    else:
                        filter = f"ORMpid_file.{filter_meta.get('column', '==')} {filter_meta.get('condition', '==')} v"
                    filters.append(eval(filter))
            with indexingSession() if db is None else nullcontext(db) as db:
                # items = db.query(ORMpid_file).filter(
                #     *(getattr(ORMpid_file, k) == v for k, v in kwargs.items())
                # ).all()
                items = db.query(ORMpid_file).filter(*filters).limit(limit).all()
        except Exception as e:
            cls._logger.exception(f"Error getting all {cls.__name__}s")
            items = []
        return [cls.from_orm(item) for item in items]

    @classmethod
    async def async_get_all(cls, limit=None, adb=None, **kwargs):
        try:
            filters = []
            for k, v in kwargs.items():
                filter_meta = cls._get_all_filter_meta.get(k, {})
                if v is not None:
                    if filter_meta.get("condition", "==") == "in":
                        v_split = v.split(",")
                        filter = f"ORMpid_file.{filter_meta.get('column', '==')}.in_(v_split)"
                    else:
                        filter = f"ORMpid_file.{filter_meta.get('column', '==')} {filter_meta.get('condition', '==')} v"
                    filters.append(eval(filter))
            async with (
                indexingAsyncSession() if adb is None else nullcontext(adb)
            ) as adb:
                # items = db.query(ORMpid_file).filter(
                #     *(getattr(ORMpid_file, k) == v for k, v in kwargs.items())
                # ).all()
                stmt = select(ORMpid_file).filter(*filters).limit(limit)
                items = (await adb.execute(stmt)).scalars().all()
        except Exception as e:
            cls._logger.exception(f"Error getting all {cls.__name__}s")
            items = []
        return [cls.from_orm(item) for item in items]

    @classmethod
    def get(cls, **kwargs):
        data = cls.get_all(**kwargs)
        if len(data) > 0:
            if len(data) > 1:
                cls._logger.warning(
                    "More than one result found, only returning the first.."
                )
            return data[0]
        else:
            return None

    @classmethod
    async def async_get(cls, **kwargs):
        data = await cls.async_get_all(**kwargs)
        if len(data) > 0:
            if len(data) > 1:
                cls._logger.warning(
                    "More than one result found, only returning the first.."
                )
            return data[0]
        else:
            return None

    @classmethod
    def get_or_create(cls, **kwargs):
        data = cls.get(**kwargs)
        if data:
            return data
        else:
            return cls(**kwargs)

    @classmethod
    def from_id(cls, id: int, db=None):
        try:
            with indexingSession() if db is None else nullcontext(db) as db:
                data = db.get(ORMpid_file, id)
            if data:
                return cls.from_orm(data)
            else:
                return None
        except Exception as e:
            cls._logger.exception(f"Error getting id {id} ({cls.__name__})")
            return None

    @classmethod
    async def async_from_id(cls, id: int, adb=None):
        try:
            async with (
                indexingAsyncSession() if adb is None else nullcontext(adb)
            ) as adb:
                data = await adb.get(ORMpid_file, id)
            if data:
                return cls.from_orm(data)
            else:
                return None
        except Exception as e:
            cls._logger.exception(f"Error getting id {id} ({cls.__name__})")
            return None

    @classmethod
    def from_dict(cls, new_obj: Dict):
        if not {"file_uuid"}.issubset(new_obj.keys()):
            raise KeyError("dict should contain at least file_uuid")
        return cls(**new_obj)

    def create(self):
        self.save()
        self._logger.debug(f"Created new pid_file with id: {self.id}")
        # try:
        #    data = self.create_all([self])
        # except Exception as e:
        #    capture_exception(e)
        #    data = None
        #    raise e
        # if data is not None:
        #    if len(data) > 0:
        #        self.__id = data[0].get("id")

    async def async_create(self):
        await self.async_save()
        self._logger.debug(f"Created new pid_file with id: {self.id}")

    @classmethod
    def bulk_upsert(cls, items: list[Self], db):
        unique_fields = cls._unique_fields if cls._unique_fields else cls._primary_keys
        if len(items) == 0:
            return []
        res = []
        for batch in itertools.batched(items, 20000):
            c = Counter()
            pre_stmt, stmt = cls.make_upsert_statement(batch)
            for x in pre_stmt:
                db.execute(x)
            r = (db.execute(stmt)).all()
            idmap = {}  # used to return the initial ordering
            for x in r:
                c[x.source] += 1
                idmap[tuple(getattr(x, k) for k in unique_fields)] = x.id
            cls._logger.info(
                f"Upserted {len(batch)} {cls.__name__}: {c['inserted']} inserts, {c['updated']} updates, {c['selected']} no-op"
            )
            r = [idmap[tuple(getattr(x, k) for k in unique_fields)] for x in batch]
            res.extend(r)
        return res

    @classmethod
    async def async_bulk_upsert(cls, items: list[Self], adb) -> list[int]:
        unique_fields = cls.unique_fields
        if len(items) == 0:
            return []
        res, res_map = [], {}
        # Sort for consistent ordering and avoiding deadlocks
        initial_order = [tuple([getattr(x, k) for k in unique_fields]) for x in items]
        items = sorted(items, key=lambda x: [getattr(x, k) for k in unique_fields])
        for batch in itertools.batched(items, 20000):
            c = Counter()
            pre_stmt, stmt = cls.make_upsert_statement(batch)
            for x in pre_stmt:
                await adb.execute(x)
            r = (await adb.execute(stmt)).all()
            idmap = {}  # used to return the initial ordering
            for x in r:
                c[x.source] += 1
                idmap[tuple(getattr(x, k) for k in unique_fields)] = x.id
            if len(idmap) < len(batch):
                # In a concurrent environment, concurrent insert might be executed at
                # the same time as this upsert statement. In that case, the execution
                # might not return a row because (1. it got inserted from a concurrent
                # transaction, and 2. the concurrent transaction hasn't committed yet)
                # Due to concurrent inserts, we might have to execute query to get the missing rows
                retries = 0
                while len(idmap) < len(batch):
                    await asyncio.sleep(10)
                    missing = [
                        x
                        for x in batch
                        if tuple(getattr(x, k) for k in unique_fields) not in idmap
                    ]
                    if not missing:
                        break
                    pre_stmt, stmt = cls.make_upsert_statement(missing)
                    for x in pre_stmt:
                        await adb.execute(x)
                    r = (await adb.execute(stmt)).all()
                    for x in r:
                        c[x.source] += 1
                        idmap[tuple(getattr(x, k) for k in unique_fields)] = x.id
                    retries += 1
                    if retries > 5:
                        cls._logger.warning(
                            f"Retried upsert {retries} times, but still missing rows"
                        )
                        break
            cls._logger.info(
                f"Upserted {len(batch)} {cls.__name__}: {c['inserted']} inserts, {c['updated']} updates, {c['selected']} no-op"
            )
            r = [idmap[tuple(getattr(x, k) for k in unique_fields)] for x in batch]
            res.extend(r)
            res_map.update(idmap)
        # Ensure correct ordering is returned using initial_order
        return [res_map[tupl] for tupl in initial_order]

    @classmethod
    def from_orm(cls, orm: ORMpid_file):
        return cls(
            **{
                field.name: getattr(orm, field.name)
                for field in inspect(ORMpid_file).c
                if isinstance(field, Column)
            }
        )

    def to_orm(self, db, safe=False):
        d = self.to_create_dict()
        if self.id:
            self_ORM = db.query(ORMpid_file).with_for_update().get(self.id)
        else:
            del d["id"]
            if safe:
                unique_data = {k: v for k, v in d.items() if k in self._unique_fields}
                self_ORM = (
                    db.query(ORMpid_file)
                    .filter_by(**unique_data)
                    .with_for_update()
                    .one_or_none()
                )
            else:
                self_ORM = None

        if not self_ORM:
            self_ORM = ORMpid_file(**d)
        else:
            for key, value in d.items():
                if key not in self._unique_fields:
                    setattr(self_ORM, key, value)

        return self_ORM

    async def async_to_orm(self, adb, safe=False):
        d = self.to_create_dict()
        if self.id:
            self_ORM = await adb.get(ORMpid_file, self.id, with_for_update=True)
        else:
            del d["id"]
            if safe:
                unique_data = {k: v for k, v in d.items() if k in self._unique_fields}
                stmt = select(ORMpid_file).filter_by(**unique_data).with_for_update()
                self_ORM = (await adb.execute(stmt)).scalar_one_or_none()
            else:
                self_ORM = None

        if not self_ORM:
            self_ORM = ORMpid_file(**d)
        else:
            for key, value in d.items():
                if key not in self._unique_fields:
                    setattr(self_ORM, key, value)

        return self_ORM

    def unsafe_safe_save(self, db=None, safe=False):
        commit = False if db else True
        unique_fields = self._unique_fields
        with indexingSession() if not db else nullcontext(db) as db:
            if self.id:
                # update
                self_ORM = db.get(ORMpid_file, self.id)
                for key, value in self.to_create_dict().items():
                    setattr(self_ORM, key, value)
            elif len(unique_fields) > 0:
                # check if exists
                d = self.to_create_dict()
                del d["id"]
                unique_data = {k: v for k, v in d.items() if k in unique_fields}
                self_ORM = (
                    db.query(ORMpid_file)
                    .filter_by(**unique_data)
                    .with_for_update()
                    .one_or_none()
                )
                if self_ORM:
                    for key, value in d.items():
                        setattr(self_ORM, key, value)
                else:
                    self_ORM = ORMpid_file(**d)
                    db.add(self_ORM)
                    db.commit()  # commit here to get the id
                self.__id = self_ORM.id
            else:
                d = self.to_create_dict()
                del d["id"]
                self_ORM = ORMpid_file(**d)
                db.add(self_ORM)
                db.commit()  # commit here to get the id
                self.__id = self_ORM.id

            if commit:
                self._logger.debug(
                    f"Committing pid_file({self.id}) to db. New: {len(db.new)}, Dirty: {len(db.dirty)}, Deleted: {len(db.deleted)}"
                )
                db.commit()
        return self

    async def async_unsafe_safe_save(self, adb=None, safe=False):
        commit = False if adb else True
        unique_fields = self._unique_fields
        async with indexingAsyncSession() if not adb else nullcontext(adb) as adb:
            if self.id:
                # update
                self_ORM = await adb.get(ORMpid_file, self.id)
                for key, value in self.to_create_dict().items():
                    setattr(self_ORM, key, value)
            elif len(unique_fields) > 0:
                # check if exists
                d = self.to_create_dict()
                del d["id"]
                unique_data = {k: v for k, v in d.items() if k in unique_fields}
                stmt = select(ORMpid_file).filter_by(**unique_data).with_for_update()
                self_ORM = (await adb.execute(stmt)).scalar_one_or_none()
                if self_ORM:
                    for key, value in d.items():
                        setattr(self_ORM, key, value)
                else:
                    self_ORM = ORMpid_file(**d)
                    adb.add(self_ORM)
                    await adb.flush()  # flush here to get the id
                self.__id = self_ORM.id
            else:
                d = self.to_create_dict()
                del d["id"]
                self_ORM = ORMpid_file(**d)
                adb.add(self_ORM)
                await adb.flush()  # flush here to get the id
                self.__id = self_ORM.id

            if commit:
                self._logger.debug(
                    f"Committing pid_file({self.id}) to db. New: {len(adb.new)}, Dirty: {len(adb.dirty)}, Deleted: {len(adb.deleted)}"
                )
                await adb.commit()
        return self

    def save(self, db=None):
        start = time.time()
        try:
            res = self.unsafe_safe_save(db=db)
        except IntegrityError as e:
            self._logger.warning(
                f"IntegrityError while saving pid_file({self.id}) to db. Trying safe save."
            )
            res = self.unsafe_safe_save(db=db, safe=True)
        self._logger.info(
            f"Saved pid_file({self.id}) to db in {time.time() - start:.3f} seconds"
        )
        return res

    async def async_save(self, adb=None):
        start = time.time()
        try:
            res = await self.async_unsafe_safe_save(adb=adb)
        except IntegrityError as e:
            self._logger.warning(
                f"IntegrityError while saving pid_file({self.id}) to db. Trying safe save."
            )
            res = await self.async_unsafe_safe_save(adb=adb, safe=True)
        self._logger.info(
            f"Saved pid_file({self.id}) to db in {time.time() - start:.3f} seconds"
        )
        return res

    @classmethod
    def create_all(cls, items, fk_column=None, fk_id=None):
        path = f"/pid_file/all"
        all_data = []
        try:
            for chunk in logging_tqdm(
                iterable=split(items, 100),
                desc="saving to pid_file",
                mininterval=10,
                total=1 + (len(items) - 1) // 100,
                leave=False,
                logger=cls._logger,
            ):
                chunk_items = [item.to_create_dict() for item in chunk]
                if fk_id is not None:

                    def set_fk_id(item, column, id):
                        item[column] = id
                        return item

                    chunk_items = [
                        set_fk_id(item, fk_column, fk_id) for item in chunk_items
                    ]
                chunk_data = request_manager.post(path=path, data=chunk_items)
                all_data = all_data + chunk_data

            for idx, (saved_item, item) in logging_tqdm(
                enumerate(zip(all_data, items)),
                desc="deleting old children",
                mininterval=10,
                total=len(items),
                leave=False,
                logger=cls._logger,
            ):
                _id = saved_item.get("id")
            starting_position = 0
        except Exception as e:
            cls._logger.exception(f"Error creating all ({cls.__name__})")
            all_data = None
            raise e
        return all_data

    def delete(self, db=None):
        if not self.__id:
            self.__set_id()
        if self.__id:
            try:
                with indexingSession() if db is None else nullcontext(db) as db:
                    data = db.get(ORMpid_file, self.__id)
                    db.delete(data)
                    db.commit()
                self.__id = None
            except Exception as e:
                self._logger.exception(f"Error deleting {self.__id} ({self.__name__})")

        else:
            self._logger.info("Object cannot be deleted as it does not exist yet")

    async def async_delete(self, adb=None):
        if not self.__id:
            self.__set_id()
        if self.__id:
            try:
                async with (
                    indexingAsyncSession() if adb is None else nullcontext(adb)
                ) as adb:
                    data = await adb.get(ORMpid_file, self.__id)
                    await adb.delete(data)
                    await adb.commit()
                self.__id = None
            except Exception as e:
                self._logger.exception(f"Error deleting {self.__id} ({self.__name__})")

        else:
            self._logger.info("Object cannot be deleted as it does not exist yet")

    def to_dict(self):
        return dict(
            id=self.__id,
            project_id=self.__project_id,
            version=self.__version,
            file_name=self.__file_name,
            file_uuid=self.__file_uuid,
            technical_name=self.__technical_name,
            s3_key=self.__s3_key,
        )

    def to_create_dict(self):
        return dict(
            id=self.__id,
            project_id=self.__project_id,
            version=self.__version,
            file_name=self.__file_name,
            file_uuid=self.__file_uuid,
            technical_name=self.__technical_name,
            s3_key=self.__s3_key,
            modified_on=datetime.now(),
        )

    def to_update_dict(self):
        return dict(
            project_id=self.__project_id,
            version=self.__version,
            file_name=self.__file_name,
            technical_name=self.__technical_name,
            s3_key=self.__s3_key,
            modified_on=datetime.now(),
        )
