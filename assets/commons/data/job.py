"""
DO NOT EDIT! THIS IS AN AUTOGENERATED FILE!!!!!
"""

import asyncio
import functools
import itertools
import time
import traceback
from collections import Counter, defaultdict
from contextlib import nullcontext
from datetime import datetime, timezone
from typing import Dict, List, Self, Any

import numpy as np
import pandas as pd
from utils.logger import makeCustomLogger, logging_tqdm
from sqlalchemy.schema import Column
from sqlalchemy.sql import (
    delete,
    values,
    cast,
    select,
    column,
    case,
    or_,
    literal,
    and_,
)
from sqlalchemy.dialects.postgresql import insert
from sqlalchemy.exc import IntegrityError
from sqlalchemy.inspection import inspect


from core.data.SentoBase import SentoBaseData, split, request_manager
from core.database.db import Session as indexingSession
from utils.async_db import indexingAsyncSession
from utils.enums import *
from models import job as ORMjob


class job(SentoBaseData):
    _logger = makeCustomLogger("job")
    _get_all_filter_meta: dict[str, dict] = {
        "project_id": {"condition": "==", "column": "project_id"}
    }
    _fields: list[str] = [
        "id",
        "name",
        "type",
        "status",
        "project_id",
        "file_id",
        "created_at",
        "completed_at",
        "error_message",
    ]
    _primary_keys: list[str] = ["id"]
    _unique_fields: list[str] = []
    _non_unique_fields: list[str] = [
        "completed_at",
        "created_at",
        "error_message",
        "file_id",
        "name",
        "project_id",
        "status",
        "type",
    ]
    # Only use set when ordering is not important. (Is important for bulk insert)
    _nullable_fields: set[str] = set(
        {
            "completed_at",
            "created_at",
            "error_message",
            "file_id",
            "id",
            "project_id",
        }
    )
    _orm: type[ORMjob] = ORMjob

    def __init__(
        self,
        id: int = None,
        name: str = None,
        type: str = None,
        status: str = None,
        project_id: int = None,
        file_id: int = None,
        created_at: datetime = None,
        completed_at: datetime = None,
        error_message: str = None,
        *args,
        **kwargs,
    ):
        super().__init__()

        if id is None:
            self.__id = None
        else:
            self.id = id
        if name is None:
            self.__name = None
        else:
            self.name = name
        if type is None:
            self.__type = None
        else:
            self.type = type
        if status is None:
            self.__status = None
        else:
            self.status = status
        if project_id is None:
            self.__project_id = None
        else:
            self.project_id = project_id
        if file_id is None:
            self.__file_id = None
        else:
            self.file_id = file_id
        if created_at is None:
            self.__created_at = None
        else:
            self.created_at = created_at
        if completed_at is None:
            self.__completed_at = None
        else:
            self.completed_at = completed_at
        if error_message is None:
            self.__error_message = None
        else:
            self.error_message = error_message

    @property
    def id(self):
        return self.__id

    @id.setter
    def id(self, new_id):
        if not hasattr(self, "__id") or new_id is not None:
            self.__id = int(new_id) if new_id is not None else None

    @property
    def name(self):
        return self.__name

    @name.setter
    def name(self, new_name):
        if not hasattr(self, "__name") or new_name is not None:
            self.__name = new_name

    @property
    def type(self):
        return self.__type

    @type.setter
    def type(self, new_type):
        if not hasattr(self, "__type") or new_type is not None:
            self.__type = new_type

    @property
    def status(self):
        return self.__status

    @status.setter
    def status(self, new_status):
        if not hasattr(self, "__status") or new_status is not None:
            self.__status = new_status

    @property
    def project_id(self):
        return self.__project_id

    @project_id.setter
    def project_id(self, new_project_id):
        if not hasattr(self, "__project_id") or new_project_id is not None:
            self.__project_id = (
                int(new_project_id) if new_project_id is not None else None
            )

    @property
    def file_id(self):
        return self.__file_id

    @file_id.setter
    def file_id(self, new_file_id):
        if not hasattr(self, "__file_id") or new_file_id is not None:
            self.__file_id = int(new_file_id) if new_file_id is not None else None

    @property
    def created_at(self):
        return self.__created_at

    @created_at.setter
    def created_at(self, new_created_at):
        if not hasattr(self, "__created_at") or new_created_at is not None:
            if isinstance(new_created_at, str):
                new_created_at = datetime.fromisoformat(new_created_at)
            elif isinstance(new_created_at, pd.Timestamp):
                new_created_at = new_created_at.to_pydatetime()
            if new_created_at.tzinfo is None:
                new_created_at = new_created_at.replace(tzinfo=timezone.utc)
            self.__created_at = new_created_at

    @property
    def completed_at(self):
        return self.__completed_at

    @completed_at.setter
    def completed_at(self, new_completed_at):
        if not hasattr(self, "__completed_at") or new_completed_at is not None:
            if isinstance(new_completed_at, str):
                new_completed_at = datetime.fromisoformat(new_completed_at)
            elif isinstance(new_completed_at, pd.Timestamp):
                new_completed_at = new_completed_at.to_pydatetime()
            if new_completed_at.tzinfo is None:
                new_completed_at = new_completed_at.replace(tzinfo=timezone.utc)
            self.__completed_at = new_completed_at

    @property
    def error_message(self):
        return self.__error_message

    @error_message.setter
    def error_message(self, new_error_message):
        if not hasattr(self, "__error_message") or new_error_message is not None:
            self.__error_message = new_error_message

    @classmethod
    def get_all(cls, limit=None, db=None, **kwargs):
        try:
            filters = []
            for k, v in kwargs.items():
                filter_meta = cls._get_all_filter_meta.get(k, {})
                if v is not None:
                    if filter_meta.get("condition", "==") == "in":
                        v_split = v.split(",")
                        filter = (
                            f"ORMjob.{filter_meta.get('column', '==')}.in_(v_split)"
                        )
                    else:
                        filter = f"ORMjob.{filter_meta.get('column', '==')} {filter_meta.get('condition', '==')} v"
                    filters.append(eval(filter))
            with indexingSession() if db is None else nullcontext(db) as db:
                # items = db.query(ORMjob).filter(
                #     *(getattr(ORMjob, k) == v for k, v in kwargs.items())
                # ).all()
                items = db.query(ORMjob).filter(*filters).limit(limit).all()
        except Exception as e:
            cls._logger.exception(f"Error getting all {cls.__name__}s")
            items = []
        return [cls.from_orm(item) for item in items]

    @classmethod
    async def async_get_all(cls, limit=None, adb=None, **kwargs):
        try:
            filters = []
            for k, v in kwargs.items():
                filter_meta = cls._get_all_filter_meta.get(k, {})
                if v is not None:
                    if filter_meta.get("condition", "==") == "in":
                        v_split = v.split(",")
                        filter = (
                            f"ORMjob.{filter_meta.get('column', '==')}.in_(v_split)"
                        )
                    else:
                        filter = f"ORMjob.{filter_meta.get('column', '==')} {filter_meta.get('condition', '==')} v"
                    filters.append(eval(filter))
            async with (
                indexingAsyncSession() if adb is None else nullcontext(adb)
            ) as adb:
                # items = db.query(ORMjob).filter(
                #     *(getattr(ORMjob, k) == v for k, v in kwargs.items())
                # ).all()
                stmt = select(ORMjob).filter(*filters).limit(limit)
                items = (await adb.execute(stmt)).scalars().all()
        except Exception as e:
            cls._logger.exception(f"Error getting all {cls.__name__}s")
            items = []
        return [cls.from_orm(item) for item in items]

    @classmethod
    def get(cls, **kwargs):
        data = cls.get_all(**kwargs)
        if len(data) > 0:
            if len(data) > 1:
                cls._logger.warning(
                    "More than one result found, only returning the first.."
                )
            return data[0]
        else:
            return None

    @classmethod
    async def async_get(cls, **kwargs):
        data = await cls.async_get_all(**kwargs)
        if len(data) > 0:
            if len(data) > 1:
                cls._logger.warning(
                    "More than one result found, only returning the first.."
                )
            return data[0]
        else:
            return None

    @classmethod
    def get_or_create(cls, **kwargs):
        data = cls.get(**kwargs)
        if data:
            return data
        else:
            return cls(**kwargs)

    @classmethod
    def from_id(cls, id: int, db=None):
        try:
            with indexingSession() if db is None else nullcontext(db) as db:
                data = db.get(ORMjob, id)
            if data:
                return cls.from_orm(data)
            else:
                return None
        except Exception as e:
            cls._logger.exception(f"Error getting id {id} ({cls.__name__})")
            return None

    @classmethod
    async def async_from_id(cls, id: int, adb=None):
        try:
            async with (
                indexingAsyncSession() if adb is None else nullcontext(adb)
            ) as adb:
                data = await adb.get(ORMjob, id)
            if data:
                return cls.from_orm(data)
            else:
                return None
        except Exception as e:
            cls._logger.exception(f"Error getting id {id} ({cls.__name__})")
            return None

    @classmethod
    def from_dict(cls, new_obj: Dict):
        if not {""}.issubset(new_obj.keys()):
            raise KeyError("dict should contain at least ")
        return cls(**new_obj)

    def create(self):
        self.save()
        self._logger.debug(f"Created new job with id: {self.id}")
        # try:
        #    data = self.create_all([self])
        # except Exception as e:
        #    capture_exception(e)
        #    data = None
        #    raise e
        # if data is not None:
        #    if len(data) > 0:
        #        self.__id = data[0].get("id")

    async def async_create(self):
        await self.async_save()
        self._logger.debug(f"Created new job with id: {self.id}")

    @classmethod
    def bulk_upsert(cls, items: list[Self], db):
        unique_fields = cls._unique_fields if cls._unique_fields else cls._primary_keys
        if len(items) == 0:
            return []
        res = []
        for batch in itertools.batched(items, 20000):
            c = Counter()
            pre_stmt, stmt = cls.make_upsert_statement(batch)
            for x in pre_stmt:
                db.execute(x)
            r = (db.execute(stmt)).all()
            idmap = {}  # used to return the initial ordering
            for x in r:
                c[x.source] += 1
                idmap[tuple(getattr(x, k) for k in unique_fields)] = x.id
            cls._logger.info(
                f"Upserted {len(batch)} {cls.__name__}: {c['inserted']} inserts, {c['updated']} updates, {c['selected']} no-op"
            )
            r = [idmap[tuple(getattr(x, k) for k in unique_fields)] for x in batch]
            res.extend(r)
        return res

    @classmethod
    async def async_bulk_upsert(cls, items: list[Self], adb) -> list[int]:
        unique_fields = cls.unique_fields
        if len(items) == 0:
            return []
        res, res_map = [], {}
        # Sort for consistent ordering and avoiding deadlocks
        initial_order = [tuple([getattr(x, k) for k in unique_fields]) for x in items]
        items = sorted(items, key=lambda x: [getattr(x, k) for k in unique_fields])
        for batch in itertools.batched(items, 20000):
            c = Counter()
            pre_stmt, stmt = cls.make_upsert_statement(batch)
            for x in pre_stmt:
                await adb.execute(x)
            r = (await adb.execute(stmt)).all()
            idmap = {}  # used to return the initial ordering
            for x in r:
                c[x.source] += 1
                idmap[tuple(getattr(x, k) for k in unique_fields)] = x.id
            if len(idmap) < len(batch):
                # In a concurrent environment, concurrent insert might be executed at
                # the same time as this upsert statement. In that case, the execution
                # might not return a row because (1. it got inserted from a concurrent
                # transaction, and 2. the concurrent transaction hasn't committed yet)
                # Due to concurrent inserts, we might have to execute query to get the missing rows
                retries = 0
                while len(idmap) < len(batch):
                    await asyncio.sleep(10)
                    missing = [
                        x
                        for x in batch
                        if tuple(getattr(x, k) for k in unique_fields) not in idmap
                    ]
                    if not missing:
                        break
                    pre_stmt, stmt = cls.make_upsert_statement(missing)
                    for x in pre_stmt:
                        await adb.execute(x)
                    r = (await adb.execute(stmt)).all()
                    for x in r:
                        c[x.source] += 1
                        idmap[tuple(getattr(x, k) for k in unique_fields)] = x.id
                    retries += 1
                    if retries > 5:
                        cls._logger.warning(
                            f"Retried upsert {retries} times, but still missing rows"
                        )
                        break
            cls._logger.info(
                f"Upserted {len(batch)} {cls.__name__}: {c['inserted']} inserts, {c['updated']} updates, {c['selected']} no-op"
            )
            r = [idmap[tuple(getattr(x, k) for k in unique_fields)] for x in batch]
            res.extend(r)
            res_map.update(idmap)
        # Ensure correct ordering is returned using initial_order
        return [res_map[tupl] for tupl in initial_order]

    @classmethod
    def from_orm(cls, orm: ORMjob):
        return cls(
            **{
                field.name: getattr(orm, field.name)
                for field in inspect(ORMjob).c
                if isinstance(field, Column)
            }
        )

    def to_orm(self, db, safe=False):
        d = self.to_create_dict()
        if self.id:
            self_ORM = db.query(ORMjob).with_for_update().get(self.id)
        else:
            del d["id"]
            if safe:
                unique_data = {k: v for k, v in d.items() if k in self._unique_fields}
                self_ORM = (
                    db.query(ORMjob)
                    .filter_by(**unique_data)
                    .with_for_update()
                    .one_or_none()
                )
            else:
                self_ORM = None

        if not self_ORM:
            self_ORM = ORMjob(**d)
        else:
            for key, value in d.items():
                if key not in self._unique_fields:
                    setattr(self_ORM, key, value)

        return self_ORM

    async def async_to_orm(self, adb, safe=False):
        d = self.to_create_dict()
        if self.id:
            self_ORM = await adb.get(ORMjob, self.id, with_for_update=True)
        else:
            del d["id"]
            if safe:
                unique_data = {k: v for k, v in d.items() if k in self._unique_fields}
                stmt = select(ORMjob).filter_by(**unique_data).with_for_update()
                self_ORM = (await adb.execute(stmt)).scalar_one_or_none()
            else:
                self_ORM = None

        if not self_ORM:
            self_ORM = ORMjob(**d)
        else:
            for key, value in d.items():
                if key not in self._unique_fields:
                    setattr(self_ORM, key, value)

        return self_ORM

    def unsafe_safe_save(self, db=None, safe=False):
        commit = False if db else True
        unique_fields = self._unique_fields
        with indexingSession() if not db else nullcontext(db) as db:
            if self.id:
                # update
                self_ORM = db.get(ORMjob, self.id)
                for key, value in self.to_create_dict().items():
                    setattr(self_ORM, key, value)
            elif len(unique_fields) > 0:
                # check if exists
                d = self.to_create_dict()
                del d["id"]
                unique_data = {k: v for k, v in d.items() if k in unique_fields}
                self_ORM = (
                    db.query(ORMjob)
                    .filter_by(**unique_data)
                    .with_for_update()
                    .one_or_none()
                )
                if self_ORM:
                    for key, value in d.items():
                        setattr(self_ORM, key, value)
                else:
                    self_ORM = ORMjob(**d)
                    db.add(self_ORM)
                    db.commit()  # commit here to get the id
                self.__id = self_ORM.id
            else:
                d = self.to_create_dict()
                del d["id"]
                self_ORM = ORMjob(**d)
                db.add(self_ORM)
                db.commit()  # commit here to get the id
                self.__id = self_ORM.id

            if commit:
                self._logger.debug(
                    f"Committing job({self.id}) to db. New: {len(db.new)}, Dirty: {len(db.dirty)}, Deleted: {len(db.deleted)}"
                )
                db.commit()
        return self

    async def async_unsafe_safe_save(self, adb=None, safe=False):
        commit = False if adb else True
        unique_fields = self._unique_fields
        async with indexingAsyncSession() if not adb else nullcontext(adb) as adb:
            if self.id:
                # update
                self_ORM = await adb.get(ORMjob, self.id)
                for key, value in self.to_create_dict().items():
                    setattr(self_ORM, key, value)
            elif len(unique_fields) > 0:
                # check if exists
                d = self.to_create_dict()
                del d["id"]
                unique_data = {k: v for k, v in d.items() if k in unique_fields}
                stmt = select(ORMjob).filter_by(**unique_data).with_for_update()
                self_ORM = (await adb.execute(stmt)).scalar_one_or_none()
                if self_ORM:
                    for key, value in d.items():
                        setattr(self_ORM, key, value)
                else:
                    self_ORM = ORMjob(**d)
                    adb.add(self_ORM)
                    await adb.flush()  # flush here to get the id
                self.__id = self_ORM.id
            else:
                d = self.to_create_dict()
                del d["id"]
                self_ORM = ORMjob(**d)
                adb.add(self_ORM)
                await adb.flush()  # flush here to get the id
                self.__id = self_ORM.id

            if commit:
                self._logger.debug(
                    f"Committing job({self.id}) to db. New: {len(adb.new)}, Dirty: {len(adb.dirty)}, Deleted: {len(adb.deleted)}"
                )
                await adb.commit()
        return self

    def save(self, db=None):
        start = time.time()
        try:
            res = self.unsafe_safe_save(db=db)
        except IntegrityError as e:
            self._logger.warning(
                f"IntegrityError while saving job({self.id}) to db. Trying safe save."
            )
            res = self.unsafe_safe_save(db=db, safe=True)
        self._logger.info(
            f"Saved job({self.id}) to db in {time.time() - start:.3f} seconds"
        )
        return res

    async def async_save(self, adb=None):
        start = time.time()
        try:
            res = await self.async_unsafe_safe_save(adb=adb)
        except IntegrityError as e:
            self._logger.warning(
                f"IntegrityError while saving job({self.id}) to db. Trying safe save."
            )
            res = await self.async_unsafe_safe_save(adb=adb, safe=True)
        self._logger.info(
            f"Saved job({self.id}) to db in {time.time() - start:.3f} seconds"
        )
        return res

    @classmethod
    def create_all(cls, items, fk_column=None, fk_id=None):
        path = f"/job/all"
        all_data = []
        try:
            for chunk in logging_tqdm(
                iterable=split(items, 100),
                desc="saving to job",
                mininterval=10,
                total=1 + (len(items) - 1) // 100,
                leave=False,
                logger=cls._logger,
            ):
                chunk_items = [item.to_create_dict() for item in chunk]
                if fk_id is not None:

                    def set_fk_id(item, column, id):
                        item[column] = id
                        return item

                    chunk_items = [
                        set_fk_id(item, fk_column, fk_id) for item in chunk_items
                    ]
                chunk_data = request_manager.post(path=path, data=chunk_items)
                all_data = all_data + chunk_data

            for idx, (saved_item, item) in logging_tqdm(
                enumerate(zip(all_data, items)),
                desc="deleting old children",
                mininterval=10,
                total=len(items),
                leave=False,
                logger=cls._logger,
            ):
                _id = saved_item.get("id")
            starting_position = 0
        except Exception as e:
            cls._logger.exception(f"Error creating all ({cls.__name__})")
            all_data = None
            raise e
        return all_data

    def delete(self, db=None):
        if not self.__id:
            self.__set_id()
        if self.__id:
            try:
                with indexingSession() if db is None else nullcontext(db) as db:
                    data = db.get(ORMjob, self.__id)
                    db.delete(data)
                    db.commit()
                self.__id = None
            except Exception as e:
                self._logger.exception(f"Error deleting {self.__id} ({self.__name__})")

        else:
            self._logger.info("Object cannot be deleted as it does not exist yet")

    async def async_delete(self, adb=None):
        if not self.__id:
            self.__set_id()
        if self.__id:
            try:
                async with (
                    indexingAsyncSession() if adb is None else nullcontext(adb)
                ) as adb:
                    data = await adb.get(ORMjob, self.__id)
                    await adb.delete(data)
                    await adb.commit()
                self.__id = None
            except Exception as e:
                self._logger.exception(f"Error deleting {self.__id} ({self.__name__})")

        else:
            self._logger.info("Object cannot be deleted as it does not exist yet")

    def to_dict(self):
        return dict(
            id=self.__id,
            name=self.__name,
            type=self.__type,
            status=self.__status,
            project_id=self.__project_id,
            file_id=self.__file_id,
            created_at=self.__created_at,
            completed_at=self.__completed_at,
            error_message=self.__error_message,
        )

    def to_create_dict(self):
        return dict(
            id=self.__id,
            name=self.__name,
            type=self.__type,
            status=self.__status,
            project_id=self.__project_id,
            file_id=self.__file_id,
            created_at=self.__created_at,
            completed_at=self.__completed_at,
            error_message=self.__error_message,
            modified_on=datetime.now(),
        )

    def to_update_dict(self):
        return dict(
            name=self.__name,
            type=self.__type,
            status=self.__status,
            project_id=self.__project_id,
            file_id=self.__file_id,
            created_at=self.__created_at,
            completed_at=self.__completed_at,
            error_message=self.__error_message,
            modified_on=datetime.now(),
        )
