"""
DO NOT EDIT! THIS IS AN AUTOGENERATED FILE!!!!!
"""

import asyncio
import functools
import itertools
import time
import traceback
from collections import Counter, defaultdict
from contextlib import nullcontext
from datetime import datetime, timezone
from typing import Dict, List, Self, Any

import numpy as np
import pandas as pd
from utils.logger import makeCustomLogger, logging_tqdm
from sqlalchemy.schema import Column
from sqlalchemy.sql import (
    delete,
    values,
    cast,
    select,
    column,
    case,
    or_,
    literal,
    and_,
)
from sqlalchemy.dialects.postgresql import insert
from sqlalchemy.exc import IntegrityError
from sqlalchemy.inspection import inspect


from core.data.SentoBase import SentoBaseData, split, request_manager
from core.database.db import Session as indexingSession
from utils.async_db import indexingAsyncSession
from utils.enums import *
from models import pid_file_link as ORMpid_file_link


class pid_file_link(SentoBaseData):
    _logger = makeCustomLogger("pid_file_link")
    _get_all_filter_meta: dict[str, dict] = {
        "pid_file_id": {"condition": "==", "column": "pid_file_id"},
        "name": {"condition": "==", "column": "name"},
        "pid_file_page_id": {"condition": "==", "column": "pid_file_page_id"},
        "type": {"condition": "==", "column": "type"},
    }
    _fields: list[str] = [
        "id",
        "pid_file_page_id",
        "pid_file_id",
        "linked_pid_file_id",
        "type",
        "name",
        "x0",
        "y0",
        "x1",
        "y1",
        "image_s3_key",
    ]
    _primary_keys: list[str] = ["id"]
    _unique_fields: list[str] = ["pid_file_id", "name", "pid_file_page_id", "type"]
    _non_unique_fields: list[str] = [
        "image_s3_key",
        "linked_pid_file_id",
        "x0",
        "x1",
        "y0",
        "y1",
    ]
    # Only use set when ordering is not important. (Is important for bulk insert)
    _nullable_fields: set[str] = set(
        {
            "id",
            "image_s3_key",
            "linked_pid_file_id",
        }
    )
    _orm: type[ORMpid_file_link] = ORMpid_file_link

    def __init__(
        self,
        id: int = None,
        pid_file_page_id: int = None,
        pid_file_id: int = None,
        linked_pid_file_id: int = None,
        type: str = None,
        name: str = None,
        x0: int = None,
        y0: int = None,
        x1: int = None,
        y1: int = None,
        image_s3_key: str = None,
        *args,
        **kwargs,
    ):
        super().__init__()

        if id is None:
            self.__id = None
        else:
            self.id = id
        if pid_file_page_id is None:
            self.__pid_file_page_id = None
        else:
            self.pid_file_page_id = pid_file_page_id
        if pid_file_id is None:
            self.__pid_file_id = None
        else:
            self.pid_file_id = pid_file_id
        if linked_pid_file_id is None:
            self.__linked_pid_file_id = None
        else:
            self.linked_pid_file_id = linked_pid_file_id
        if type is None:
            self.__type = None
        else:
            self.type = type
        if name is None:
            self.__name = None
        else:
            self.name = name
        if x0 is None:
            self.__x0 = None
        else:
            self.x0 = x0
        if y0 is None:
            self.__y0 = None
        else:
            self.y0 = y0
        if x1 is None:
            self.__x1 = None
        else:
            self.x1 = x1
        if y1 is None:
            self.__y1 = None
        else:
            self.y1 = y1
        if image_s3_key is None:
            self.__image_s3_key = None
        else:
            self.image_s3_key = image_s3_key

    @property
    def id(self):
        return self.__id

    @id.setter
    def id(self, new_id):
        if not hasattr(self, "__id") or new_id is not None:
            self.__id = int(new_id) if new_id is not None else None

    @property
    def pid_file_page_id(self):
        return self.__pid_file_page_id

    @pid_file_page_id.setter
    def pid_file_page_id(self, new_pid_file_page_id):
        if not hasattr(self, "__pid_file_page_id") or new_pid_file_page_id is not None:
            self.__pid_file_page_id = (
                int(new_pid_file_page_id) if new_pid_file_page_id is not None else None
            )

    @property
    def pid_file_id(self):
        return self.__pid_file_id

    @pid_file_id.setter
    def pid_file_id(self, new_pid_file_id):
        if not hasattr(self, "__pid_file_id") or new_pid_file_id is not None:
            self.__pid_file_id = (
                int(new_pid_file_id) if new_pid_file_id is not None else None
            )

    @property
    def linked_pid_file_id(self):
        return self.__linked_pid_file_id

    @linked_pid_file_id.setter
    def linked_pid_file_id(self, new_linked_pid_file_id):
        if (
            not hasattr(self, "__linked_pid_file_id")
            or new_linked_pid_file_id is not None
        ):
            self.__linked_pid_file_id = (
                int(new_linked_pid_file_id)
                if new_linked_pid_file_id is not None
                else None
            )

    @property
    def type(self):
        return self.__type

    @type.setter
    def type(self, new_type):
        if not hasattr(self, "__type") or new_type is not None:
            self.__type = new_type

    @property
    def name(self):
        return self.__name

    @name.setter
    def name(self, new_name):
        if not hasattr(self, "__name") or new_name is not None:
            self.__name = new_name

    @property
    def x0(self):
        return self.__x0

    @x0.setter
    def x0(self, new_x0):
        if not hasattr(self, "__x0") or new_x0 is not None:
            self.__x0 = int(new_x0) if new_x0 is not None else None

    @property
    def y0(self):
        return self.__y0

    @y0.setter
    def y0(self, new_y0):
        if not hasattr(self, "__y0") or new_y0 is not None:
            self.__y0 = int(new_y0) if new_y0 is not None else None

    @property
    def x1(self):
        return self.__x1

    @x1.setter
    def x1(self, new_x1):
        if not hasattr(self, "__x1") or new_x1 is not None:
            self.__x1 = int(new_x1) if new_x1 is not None else None

    @property
    def y1(self):
        return self.__y1

    @y1.setter
    def y1(self, new_y1):
        if not hasattr(self, "__y1") or new_y1 is not None:
            self.__y1 = int(new_y1) if new_y1 is not None else None

    @property
    def image_s3_key(self):
        return self.__image_s3_key

    @image_s3_key.setter
    def image_s3_key(self, new_image_s3_key):
        if not hasattr(self, "__image_s3_key") or new_image_s3_key is not None:
            self.__image_s3_key = new_image_s3_key

    @classmethod
    def get_all(cls, limit=None, db=None, **kwargs):
        try:
            filters = []
            for k, v in kwargs.items():
                filter_meta = cls._get_all_filter_meta.get(k, {})
                if v is not None:
                    if filter_meta.get("condition", "==") == "in":
                        v_split = v.split(",")
                        filter = f"ORMpid_file_link.{filter_meta.get('column', '==')}.in_(v_split)"
                    else:
                        filter = f"ORMpid_file_link.{filter_meta.get('column', '==')} {filter_meta.get('condition', '==')} v"
                    filters.append(eval(filter))
            with indexingSession() if db is None else nullcontext(db) as db:
                # items = db.query(ORMpid_file_link).filter(
                #     *(getattr(ORMpid_file_link, k) == v for k, v in kwargs.items())
                # ).all()
                items = db.query(ORMpid_file_link).filter(*filters).limit(limit).all()
        except Exception as e:
            cls._logger.exception(f"Error getting all {cls.__name__}s")
            items = []
        return [cls.from_orm(item) for item in items]

    @classmethod
    async def async_get_all(cls, limit=None, adb=None, **kwargs):
        try:
            filters = []
            for k, v in kwargs.items():
                filter_meta = cls._get_all_filter_meta.get(k, {})
                if v is not None:
                    if filter_meta.get("condition", "==") == "in":
                        v_split = v.split(",")
                        filter = f"ORMpid_file_link.{filter_meta.get('column', '==')}.in_(v_split)"
                    else:
                        filter = f"ORMpid_file_link.{filter_meta.get('column', '==')} {filter_meta.get('condition', '==')} v"
                    filters.append(eval(filter))
            async with (
                indexingAsyncSession() if adb is None else nullcontext(adb)
            ) as adb:
                # items = db.query(ORMpid_file_link).filter(
                #     *(getattr(ORMpid_file_link, k) == v for k, v in kwargs.items())
                # ).all()
                stmt = select(ORMpid_file_link).filter(*filters).limit(limit)
                items = (await adb.execute(stmt)).scalars().all()
        except Exception as e:
            cls._logger.exception(f"Error getting all {cls.__name__}s")
            items = []
        return [cls.from_orm(item) for item in items]

    @classmethod
    def get(cls, **kwargs):
        data = cls.get_all(**kwargs)
        if len(data) > 0:
            if len(data) > 1:
                cls._logger.warning(
                    "More than one result found, only returning the first.."
                )
            return data[0]
        else:
            return None

    @classmethod
    async def async_get(cls, **kwargs):
        data = await cls.async_get_all(**kwargs)
        if len(data) > 0:
            if len(data) > 1:
                cls._logger.warning(
                    "More than one result found, only returning the first.."
                )
            return data[0]
        else:
            return None

    @classmethod
    def get_or_create(cls, **kwargs):
        data = cls.get(**kwargs)
        if data:
            return data
        else:
            return cls(**kwargs)

    @classmethod
    def from_id(cls, id: int, db=None):
        try:
            with indexingSession() if db is None else nullcontext(db) as db:
                data = db.get(ORMpid_file_link, id)
            if data:
                return cls.from_orm(data)
            else:
                return None
        except Exception as e:
            cls._logger.exception(f"Error getting id {id} ({cls.__name__})")
            return None

    @classmethod
    async def async_from_id(cls, id: int, adb=None):
        try:
            async with (
                indexingAsyncSession() if adb is None else nullcontext(adb)
            ) as adb:
                data = await adb.get(ORMpid_file_link, id)
            if data:
                return cls.from_orm(data)
            else:
                return None
        except Exception as e:
            cls._logger.exception(f"Error getting id {id} ({cls.__name__})")
            return None

    @classmethod
    def from_dict(cls, new_obj: Dict):
        if not {"pid_file_page_id", "pid_file_id", "type", "name"}.issubset(
            new_obj.keys()
        ):
            raise KeyError(
                "dict should contain at least pid_file_id,name,pid_file_page_id,type"
            )
        return cls(**new_obj)

    def create(self):
        self.save()
        self._logger.debug(f"Created new pid_file_link with id: {self.id}")
        # try:
        #    data = self.create_all([self])
        # except Exception as e:
        #    capture_exception(e)
        #    data = None
        #    raise e
        # if data is not None:
        #    if len(data) > 0:
        #        self.__id = data[0].get("id")

    async def async_create(self):
        await self.async_save()
        self._logger.debug(f"Created new pid_file_link with id: {self.id}")

    @classmethod
    def bulk_upsert(cls, items: list[Self], db):
        unique_fields = cls._unique_fields if cls._unique_fields else cls._primary_keys
        if len(items) == 0:
            return []
        res = []
        for batch in itertools.batched(items, 20000):
            c = Counter()
            pre_stmt, stmt = cls.make_upsert_statement(batch)
            for x in pre_stmt:
                db.execute(x)
            r = (db.execute(stmt)).all()
            idmap = {}  # used to return the initial ordering
            for x in r:
                c[x.source] += 1
                idmap[tuple(getattr(x, k) for k in unique_fields)] = x.id
            cls._logger.info(
                f"Upserted {len(batch)} {cls.__name__}: {c['inserted']} inserts, {c['updated']} updates, {c['selected']} no-op"
            )
            r = [idmap[tuple(getattr(x, k) for k in unique_fields)] for x in batch]
            res.extend(r)
        return res

    @classmethod
    async def async_bulk_upsert(cls, items: list[Self], adb) -> list[int]:
        unique_fields = cls.unique_fields
        if len(items) == 0:
            return []
        res, res_map = [], {}
        # Sort for consistent ordering and avoiding deadlocks
        initial_order = [tuple([getattr(x, k) for k in unique_fields]) for x in items]
        items = sorted(items, key=lambda x: [getattr(x, k) for k in unique_fields])
        for batch in itertools.batched(items, 20000):
            c = Counter()
            pre_stmt, stmt = cls.make_upsert_statement(batch)
            for x in pre_stmt:
                await adb.execute(x)
            r = (await adb.execute(stmt)).all()
            idmap = {}  # used to return the initial ordering
            for x in r:
                c[x.source] += 1
                idmap[tuple(getattr(x, k) for k in unique_fields)] = x.id
            if len(idmap) < len(batch):
                # In a concurrent environment, concurrent insert might be executed at
                # the same time as this upsert statement. In that case, the execution
                # might not return a row because (1. it got inserted from a concurrent
                # transaction, and 2. the concurrent transaction hasn't committed yet)
                # Due to concurrent inserts, we might have to execute query to get the missing rows
                retries = 0
                while len(idmap) < len(batch):
                    await asyncio.sleep(10)
                    missing = [
                        x
                        for x in batch
                        if tuple(getattr(x, k) for k in unique_fields) not in idmap
                    ]
                    if not missing:
                        break
                    pre_stmt, stmt = cls.make_upsert_statement(missing)
                    for x in pre_stmt:
                        await adb.execute(x)
                    r = (await adb.execute(stmt)).all()
                    for x in r:
                        c[x.source] += 1
                        idmap[tuple(getattr(x, k) for k in unique_fields)] = x.id
                    retries += 1
                    if retries > 5:
                        cls._logger.warning(
                            f"Retried upsert {retries} times, but still missing rows"
                        )
                        break
            cls._logger.info(
                f"Upserted {len(batch)} {cls.__name__}: {c['inserted']} inserts, {c['updated']} updates, {c['selected']} no-op"
            )
            r = [idmap[tuple(getattr(x, k) for k in unique_fields)] for x in batch]
            res.extend(r)
            res_map.update(idmap)
        # Ensure correct ordering is returned using initial_order
        return [res_map[tupl] for tupl in initial_order]

    @classmethod
    def from_orm(cls, orm: ORMpid_file_link):
        return cls(
            **{
                field.name: getattr(orm, field.name)
                for field in inspect(ORMpid_file_link).c
                if isinstance(field, Column)
            }
        )

    def to_orm(self, db, safe=False):
        d = self.to_create_dict()
        if self.id:
            self_ORM = db.query(ORMpid_file_link).with_for_update().get(self.id)
        else:
            del d["id"]
            if safe:
                unique_data = {k: v for k, v in d.items() if k in self._unique_fields}
                self_ORM = (
                    db.query(ORMpid_file_link)
                    .filter_by(**unique_data)
                    .with_for_update()
                    .one_or_none()
                )
            else:
                self_ORM = None

        if not self_ORM:
            self_ORM = ORMpid_file_link(**d)
        else:
            for key, value in d.items():
                if key not in self._unique_fields:
                    setattr(self_ORM, key, value)

        return self_ORM

    async def async_to_orm(self, adb, safe=False):
        d = self.to_create_dict()
        if self.id:
            self_ORM = await adb.get(ORMpid_file_link, self.id, with_for_update=True)
        else:
            del d["id"]
            if safe:
                unique_data = {k: v for k, v in d.items() if k in self._unique_fields}
                stmt = (
                    select(ORMpid_file_link).filter_by(**unique_data).with_for_update()
                )
                self_ORM = (await adb.execute(stmt)).scalar_one_or_none()
            else:
                self_ORM = None

        if not self_ORM:
            self_ORM = ORMpid_file_link(**d)
        else:
            for key, value in d.items():
                if key not in self._unique_fields:
                    setattr(self_ORM, key, value)

        return self_ORM

    def unsafe_safe_save(self, db=None, safe=False):
        commit = False if db else True
        unique_fields = self._unique_fields
        with indexingSession() if not db else nullcontext(db) as db:
            if self.id:
                # update
                self_ORM = db.get(ORMpid_file_link, self.id)
                for key, value in self.to_create_dict().items():
                    setattr(self_ORM, key, value)
            elif len(unique_fields) > 0:
                # check if exists
                d = self.to_create_dict()
                del d["id"]
                unique_data = {k: v for k, v in d.items() if k in unique_fields}
                self_ORM = (
                    db.query(ORMpid_file_link)
                    .filter_by(**unique_data)
                    .with_for_update()
                    .one_or_none()
                )
                if self_ORM:
                    for key, value in d.items():
                        setattr(self_ORM, key, value)
                else:
                    self_ORM = ORMpid_file_link(**d)
                    db.add(self_ORM)
                    db.commit()  # commit here to get the id
                self.__id = self_ORM.id
            else:
                d = self.to_create_dict()
                del d["id"]
                self_ORM = ORMpid_file_link(**d)
                db.add(self_ORM)
                db.commit()  # commit here to get the id
                self.__id = self_ORM.id

            if commit:
                self._logger.debug(
                    f"Committing pid_file_link({self.id}) to db. New: {len(db.new)}, Dirty: {len(db.dirty)}, Deleted: {len(db.deleted)}"
                )
                db.commit()
        return self

    async def async_unsafe_safe_save(self, adb=None, safe=False):
        commit = False if adb else True
        unique_fields = self._unique_fields
        async with indexingAsyncSession() if not adb else nullcontext(adb) as adb:
            if self.id:
                # update
                self_ORM = await adb.get(ORMpid_file_link, self.id)
                for key, value in self.to_create_dict().items():
                    setattr(self_ORM, key, value)
            elif len(unique_fields) > 0:
                # check if exists
                d = self.to_create_dict()
                del d["id"]
                unique_data = {k: v for k, v in d.items() if k in unique_fields}
                stmt = (
                    select(ORMpid_file_link).filter_by(**unique_data).with_for_update()
                )
                self_ORM = (await adb.execute(stmt)).scalar_one_or_none()
                if self_ORM:
                    for key, value in d.items():
                        setattr(self_ORM, key, value)
                else:
                    self_ORM = ORMpid_file_link(**d)
                    adb.add(self_ORM)
                    await adb.flush()  # flush here to get the id
                self.__id = self_ORM.id
            else:
                d = self.to_create_dict()
                del d["id"]
                self_ORM = ORMpid_file_link(**d)
                adb.add(self_ORM)
                await adb.flush()  # flush here to get the id
                self.__id = self_ORM.id

            if commit:
                self._logger.debug(
                    f"Committing pid_file_link({self.id}) to db. New: {len(adb.new)}, Dirty: {len(adb.dirty)}, Deleted: {len(adb.deleted)}"
                )
                await adb.commit()
        return self

    def save(self, db=None):
        start = time.time()
        try:
            res = self.unsafe_safe_save(db=db)
        except IntegrityError as e:
            self._logger.warning(
                f"IntegrityError while saving pid_file_link({self.id}) to db. Trying safe save."
            )
            res = self.unsafe_safe_save(db=db, safe=True)
        self._logger.info(
            f"Saved pid_file_link({self.id}) to db in {time.time() - start:.3f} seconds"
        )
        return res

    async def async_save(self, adb=None):
        start = time.time()
        try:
            res = await self.async_unsafe_safe_save(adb=adb)
        except IntegrityError as e:
            self._logger.warning(
                f"IntegrityError while saving pid_file_link({self.id}) to db. Trying safe save."
            )
            res = await self.async_unsafe_safe_save(adb=adb, safe=True)
        self._logger.info(
            f"Saved pid_file_link({self.id}) to db in {time.time() - start:.3f} seconds"
        )
        return res

    @classmethod
    def create_all(cls, items, fk_column=None, fk_id=None):
        path = f"/pid_file_link/all"
        all_data = []
        try:
            for chunk in logging_tqdm(
                iterable=split(items, 100),
                desc="saving to pid_file_link",
                mininterval=10,
                total=1 + (len(items) - 1) // 100,
                leave=False,
                logger=cls._logger,
            ):
                chunk_items = [item.to_create_dict() for item in chunk]
                if fk_id is not None:

                    def set_fk_id(item, column, id):
                        item[column] = id
                        return item

                    chunk_items = [
                        set_fk_id(item, fk_column, fk_id) for item in chunk_items
                    ]
                chunk_data = request_manager.post(path=path, data=chunk_items)
                all_data = all_data + chunk_data

            for idx, (saved_item, item) in logging_tqdm(
                enumerate(zip(all_data, items)),
                desc="deleting old children",
                mininterval=10,
                total=len(items),
                leave=False,
                logger=cls._logger,
            ):
                _id = saved_item.get("id")
            starting_position = 0
        except Exception as e:
            cls._logger.exception(f"Error creating all ({cls.__name__})")
            all_data = None
            raise e
        return all_data

    def delete(self, db=None):
        if not self.__id:
            self.__set_id()
        if self.__id:
            try:
                with indexingSession() if db is None else nullcontext(db) as db:
                    data = db.get(ORMpid_file_link, self.__id)
                    db.delete(data)
                    db.commit()
                self.__id = None
            except Exception as e:
                self._logger.exception(f"Error deleting {self.__id} ({self.__name__})")

        else:
            self._logger.info("Object cannot be deleted as it does not exist yet")

    async def async_delete(self, adb=None):
        if not self.__id:
            self.__set_id()
        if self.__id:
            try:
                async with (
                    indexingAsyncSession() if adb is None else nullcontext(adb)
                ) as adb:
                    data = await adb.get(ORMpid_file_link, self.__id)
                    await adb.delete(data)
                    await adb.commit()
                self.__id = None
            except Exception as e:
                self._logger.exception(f"Error deleting {self.__id} ({self.__name__})")

        else:
            self._logger.info("Object cannot be deleted as it does not exist yet")

    def to_dict(self):
        return dict(
            id=self.__id,
            pid_file_page_id=self.__pid_file_page_id,
            pid_file_id=self.__pid_file_id,
            linked_pid_file_id=self.__linked_pid_file_id,
            type=self.__type,
            name=self.__name,
            x0=self.__x0,
            y0=self.__y0,
            x1=self.__x1,
            y1=self.__y1,
            image_s3_key=self.__image_s3_key,
        )

    def to_create_dict(self):
        return dict(
            id=self.__id,
            pid_file_page_id=self.__pid_file_page_id,
            pid_file_id=self.__pid_file_id,
            linked_pid_file_id=self.__linked_pid_file_id,
            type=self.__type,
            name=self.__name,
            x0=self.__x0,
            y0=self.__y0,
            x1=self.__x1,
            y1=self.__y1,
            image_s3_key=self.__image_s3_key,
            modified_on=datetime.now(),
        )

    def to_update_dict(self):
        return dict(
            linked_pid_file_id=self.__linked_pid_file_id,
            x0=self.__x0,
            y0=self.__y0,
            x1=self.__x1,
            y1=self.__y1,
            image_s3_key=self.__image_s3_key,
            modified_on=datetime.now(),
        )
